{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"id": "Th1K4AteTNZ0"}, "outputs": [], "source": ["# Based on the Huggin Face Course Introduction : https://huggingface.co/\n", "# Modified by Abdeslem ISSAADI, Univ. Paris 8"]}, {"cell_type": "markdown", "metadata": {"id": "izSvqycGTNZ3"}, "source": ["# Transformers: Technical Introduction\n", "\n", "This notebook provides a comprehensive introduction to the Hugging Face Transformers library, focusing on various natural language processing (NLP) tasks using pre-trained language models. Participants will learn how to install and verify the library, utilize pipelines for sentiment analysis, zero-shot classification, text generation, mask filling, named entity recognition (NER), question answering, text summarization, and translation. By the end of this course, learners will be equipped to effectively leverage these pipelines for a wide range of NLP applications, enhancing their skills in modern language processing techniques."]}, {"cell_type": "markdown", "metadata": {"id": "fHZkstD2TNZ6"}, "source": ["## Installing Required Libraries\n", "\n", "First, we need to install the `transformers` library. This library is developed by Hugging Face and provides a wide range of pre-trained models for NLP tasks."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "kWbZLBZTRLuW"}, "outputs": [], "source": ["# Install the transformers library\n", "!pip install transformers"]}, {"cell_type": "markdown", "metadata": {"id": "2za3SjLxTNZ8"}, "source": ["After installing the transformers library, we will check its version to ensure it has been installed correctly."]}, {"cell_type": "code", "execution_count": 1, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "efoKDRLgRYrc", "outputId": "743a006c-b1f3-4611-dfdf-5c68f333dd90"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["4.57.1\n"]}], "source": ["# Verify the installation of the transformers library\n", "import transformers\n", "print(transformers.__version__)"]}, {"cell_type": "markdown", "metadata": {"id": "_K3ylSTrTNZ9"}, "source": ["## Working with pipelines\n", "\n", "The most basic object in the Huggin Face Transformers library is the pipeline() function.\n", "\n", "There are three main steps involved when you pass some text to a pipeline():\n", "\n", " - Text preprocessing,\n", " - Model prediction,\n", " - Output post-processing.\n", "\n", "We can directly input any text into it and get an intelligible answer.\n", "By default, this pipeline selects a particular pretrained model.\n", "\n", "Let's try to use it !\n", "\n", "## Sentiment Analysis Pipeline\n", "\n", "This pipeline is a pre-configured model that can analyze the sentiment of a given text, categorizing it as positive, negative, or neutral.\n", "\n", "Initialize the sentiment analysis pipeline. This will download the pre-trained model and tokenizer."]}, {"cell_type": "code", "execution_count": 4, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 329, "referenced_widgets": ["1df6812a893c4ce9929ec90997064bf3", "3646f7f51955466c9703390c8ac6500b", "2014880de02a4540b277460e7a67f555", "09f9c86a6de24ab5bf4fed2b3f75c03e", "b8e00a0584464755915e2c4ac18954c3", "d8fd5d84c8484741bb55bc48a557893e", "fce0b38a4e3249e382caa5acea7ebd5e", "631cf010e0704dc48b6ea5da56daebec", "60e6ffb7f81b4c00bd4c616d2c49b2c7", "f0e5a2f564ee41ee957c3c112ec3b19c", "0761c38a9af440ed99da22e54b94c002", "f59d9a8533a84ebb828add68ada5e4f7", "65e3f7b45a514fc7a41b9b3b86914f67", "6a9e450532134c819aa57ea732cfcadc", "68e2477d5a544dd4ab8b0360582ada6b", "7540bc92203b4b65b41d115d3e04ef06", "4a64cf8c54e44fa5ac457f60eeddb927", "e3e1d556f60643cbb1ba2ef21a5685d0", "4623cc45f8ad4571bdc2a19dcbf1fc8c", "91e1b154d81b445b8de76e8fec33fcc0", "b26fa631da6a421aba63030d4c691dfa", "bd7ec1e1713147bdae173ec652f371bf", "0da07732e8c24d69919d45f0c33772e6", "9f19d9f82b274e008b17d1bbfe0783e6", "221b766e47fe4e1f8b25975913f95f07", "5ad2a113cbf64fdba0459c5aeab3abfc", "eebc3b1e61f445d587eb721d1b56542f", "e88503714b1341c1b1e2a7fbf3e1e510", "79dc2de7cba54622982cbde2c0dbc3f0", "a7abb40d5ad74fc3b8e5d4e1ca43fa3d", "93ae624a38c14a8d86bd71108f3e5b3b", "0c2cd683f2dc46e09769ff2df144283b", "7c6d49c573c54f6aa3177718a5cc8a11", "2a7dda364f8349e38a42b7f29553fc5b", "916537ca73974faba44aba24ff5ec772", "eb7e68eff81c481aaa9f0d68ea354cc3", "b8a3b5c9456b4db9b7e5250b5aa4613e", "ab41fa4838df45bdb0c111823440e5b6", "ab635ea97da9469797ef107d4498e419", "5879dfc1ad9e4c2b91572cfcad54914e", "081259549731461da634a746e783ac2e", "b70a407d76b840f28026383df0f2c36a", "9e5c7ab1f5c0417d9a3bb1f58e8b99c8", "9e86fd6dac9247ca9e6da3311a204a85"]}, "id": "Zi4xe1hBTNZ9", "outputId": "6345f317-0c41-45c2-ef84-621664bedfe1"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n", "The secret `HF_TOKEN` does not exist in your Colab secrets.\n", "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n", "You will be able to reuse this secret in all of your notebooks.\n", "Please note that authentication is recommended but still optional to access public models or datasets.\n", "  warnings.warn(\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "1df6812a893c4ce9929ec90997064bf3", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "f59d9a8533a84ebb828add68ada5e4f7", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "0da07732e8c24d69919d45f0c33772e6", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "2a7dda364f8349e38a42b7f29553fc5b", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.txt: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[{'label': 'POSITIVE', 'score': 0.999885082244873}]\n"]}], "source": ["# Initialize the sentiment analysis pipeline\n", "# Importer Transformers\n", "from transformers import pipeline\n", "\n", "# Initialiser le pipeline d'analyse de sentiment\n", "sentiment_classifier = pipeline(task=\"sentiment-analysis\")\n", "\n", "# Exemple d'utilisation\n", "sentence = \"I absolutely loved the movie! It was fantastic and thrilling.\"\n", "result = sentiment_classifier(sentence)\n", "print(result)\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "EDDSQ6YtTNZ-", "outputId": "d4ed03a6-5ef7-4c20-b896-10bdfa4e2b4e"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Texte: Nice, I've been waiting for a short HuggingFace course my whole life!\n", "Sentiment: POSITIVE, Score: 0.9979\n", "\n", "Texte: I hate this so much\n", "Sentiment: NEGATIVE, Score: 0.9995\n", "\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Initialiser le pipeline d'analyse de sentiment\n", "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n", "\n", "# Liste de textes \u00e0 analyser\n", "texts = [\n", "    \"Nice, I've been waiting for a short HuggingFace course my whole life!\",\n", "    \"I hate this so much\"\n", "]\n", "\n", "# Analyser le sentiment de chaque texte\n", "results = sentiment_analyzer(texts)\n", "\n", "# Afficher les r\u00e9sultats\n", "for txt, res in zip(texts, results):\n", "    print(f\"Texte: {txt}\")\n", "    print(f\"Sentiment: {res['label']}, Score: {res['score']:.4f}\\n\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "WPW2kNgBTNZ_"}, "source": ["Each result contains:\n", "\n", "label: The predicted sentiment label (e.g., POSITIVE or NEGATIVE).\n", "score: The confidence score of the prediction.\n", "Let's break down the results for better understanding."]}, {"cell_type": "code", "execution_count": 8, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "0FtUl2t0TNZ_", "outputId": "b91c2094-eb2d-4583-e6bf-ba2bef4013ac"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Texte: Nice, I've been waiting for a short HuggingFace course my whole life!\n", "Sentiment: POSITIVE, Score: 0.9979\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Initialiser le pipeline\n", "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n", "\n", "# Exemple de texte\n", "example_text = \"Nice, I've been waiting for a short HuggingFace course my whole life!\"\n", "example_result = sentiment_analyzer(example_text)[0]\n", "\n", "# Afficher le r\u00e9sultat d\u00e9taill\u00e9\n", "print(f\"Texte: {example_text}\")\n", "print(f\"Sentiment: {example_result['label']}, Score: {example_result['score']:.4f}\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "hRvuNl2ETNaA"}, "outputs": [], "source": [">>\n", "Texte: Nice, I've been waiting for a short HuggingFace course my whole life!\n", "Sentiment: POSITIVE, Score: 0.9979"]}, {"cell_type": "markdown", "metadata": {"id": "J0ptJk_7TNaA"}, "source": ["Feel free to analyze more texts by modifying the texts list and re-running the analysis cell."]}, {"cell_type": "code", "execution_count": 9, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "rtK59eiFTNaB", "outputId": "0527aaf9-3a54-4b3b-b448-054697dbf664"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Texte: This is the best movie I have ever seen!\n", "Sentiment: POSITIVE, Score: 0.9999\n", "\n", "Texte: The product quality is terrible and I'm very disappointed.\n", "Sentiment: NEGATIVE, Score: 0.9998\n", "\n", "Texte: I'm feeling great today!\n", "Sentiment: POSITIVE, Score: 0.9999\n", "\n", "Texte: It's a gloomy and rainy day.\n", "Sentiment: NEGATIVE, Score: 0.9975\n", "\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Initialiser le pipeline\n", "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n", "\n", "# Liste de nouveaux textes \u00e0 analyser\n", "more_texts = [\n", "    \"This is the best movie I have ever seen!\",\n", "    \"The product quality is terrible and I'm very disappointed.\",\n", "    \"I'm feeling great today!\",\n", "    \"It's a gloomy and rainy day.\"\n", "]\n", "\n", "# Analyser le sentiment de chaque texte\n", "more_results = sentiment_analyzer(more_texts)\n", "\n", "# Afficher les r\u00e9sultats\n", "for txt, res in zip(more_texts, more_results):\n", "    print(f\"Texte: {txt}\")\n", "    print(f\"Sentiment: {res['label']}, Score: {res['score']:.4f}\\n\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "KMkH3BHxTNaC"}, "outputs": [], "source": [">>\n", "Texte: This is the best movie I have ever seen!\n", "Sentiment: POSITIVE, Score: 0.9999\n", "\n", "Texte: The product quality is terrible and I'm very disappointed.\n", "Sentiment: NEGATIVE, Score: 0.9998\n", "\n", "Texte: I'm feeling great today!\n", "Sentiment: POSITIVE, Score: 0.9999\n", "\n", "Texte: It's a gloomy and rainy day.\n", "Sentiment: NEGATIVE, Score: 0.9975"]}, {"cell_type": "markdown", "metadata": {"id": "9r9V4eg-TNaC"}, "source": ["## Other pipelines\n", "Some of the currently available pipelines are:\n", "\n", " - feature-extraction\n", "- fill-mask\n", "- ner (named entity recognition)\n", "- question-answering\n", "- sentiment-analysis\n", "- summarization\n", "- text-generation\n", "- translation\n", "- zero-shot-classification"]}, {"cell_type": "markdown", "metadata": {"id": "WR_OQgJOTNaD"}, "source": ["## Zero-shot classification\n", "\n", "The zero-shot-classification pipeline is very powerful for tasks where we need to classify texts that haven\u2019t been labelled. It returns probability scores for any list of labels you want!\n", "It's called zero-shot because you don\u2019t need to fine-tune the model on your data to use it."]}, {"cell_type": "markdown", "metadata": {"id": "0lxP0ChkTNaE"}, "source": ["### Initialize the Classifier\n", "\n", "We initialize the classifier using the `pipeline` function and specify `\"zero-shot-classification\"` as the task."]}, {"cell_type": "code", "execution_count": 10, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 264, "referenced_widgets": ["03cbc655e874456094c25b88e56454df", "ce7f0fd64fa8440fbb0aa193351c8b57", "dacf35a922a54a4ba7e7384162b05dc6", "73d0dba094684ceeb7cf5b9bdbcba560", "57ef1cd990934a7597b2a33e6cc4579b", "54cbdf42e2de4da0b93b2eec5e299cf8", "30e45eb9a8854269b7eda563b838a6a5", "5398f105f09b4d258cba7ffed01267f5", "7abae39192f64f28a214ad765366d8ab", "14409eb001e2406e92b57f14d1572549", "d4670b7a860047d9a8dfeedd884d1a63", "cc1857d257024b129f7c1d5b24ebdd8c", "871922c469964646ad044354cb8d96ea", "2a431f943462498eb251d4e1b48c0b7b", "ad0bfbe7f5b348f2ad8bbf450ef72fcc", "6ed08c0421a8470696c2518af33ee919", "46aafa6528624b3bbf25ea03a23236c7", "55b3f1002e6547148fbf2883bbbaa015", "ecb25c5d2a8e4c9e925834bb04b5178a", "d963f4879b504dd5885a48ed7222309b", "165577a597a94e7f9500a55159cd6b14", "b72b915159c242108c66f2893677ddbf", "5664699e31b24365b415d5f0409dac0b", "9b08ef2cca744fe895dfbf39bb3e6267", "8a6e4a5215d34293aab19dcda2fd64a5", "9545faa860c54ede934481e268dcbc64", "e77e547df065410b8c3ef869ce2da31f", "c7a32b47eb464d908d39955aa63b7f92", "e8ccb7164f764ff3b17ecdecc0960646", "3853f47660064d8eb2d47c1e779e558a", "bed8dfcf86a54ed6a4ddb33b93ef8eaf", "26c50c99144b4f179d55e07322ad5fd6", "14c794a42381433cb15caf42332f803b", "b9deb9f0852346a0aef7ed3bb9c228d6", "01c6bee220e44b04b149be38624b7838", "caae14addbdb41ca98407ca26b97f119", "e73abd1cb8664191841dac399622083a", "71314407153b444287170b06eaed6027", "e2cd153087fc4d90a785b580d08e17df", "d333a1fe151d4b0ea21c108c59e66f3e", "0651a99818cd497bb172406eeab0f859", "61fb22c35eaa4b9c8ba2d3b771599419", "4a047aa9bca54c4da3751572ae4d92e3", "ddec359ba62a42f29eb0b6efb91e97ef", "46b36bd52f2443369fbd33aa4e3e9f9f", "d8a45a2bf1384508926050754a445b09", "96c5efbfbdf1472f86487854ac62570d", "d1e437b335fb4886ac767f7748cf1bb7", "2c9b8153a2f24387b323ab357c73642e", "c8429fd2925947da9e2ca07d59deebd4", "b06887985b22417c9232f3922c6f3cfa", "14303687761c4fbfa222b34ec4a0d49e", "4390d8e523c3416fb5f1adba5047488b", "b66b85a6c80342c59f43a0f0efb9b026", "8d432ba54618478d91ac35bc4bebd9fc", "4a0ee125e7724a048e82f115a13c395a", "27efc987a5fa433585e5698d82afb471", "2713b48d34a643c7909702445c562a02", "d26d65bd46284ed595b7d99f77e0303f", "3bb6f2d1a0584ad79d6c509fa7d491b8", "dccf65a1c9a048dfa1d422229592d78a", "8376ac8a4055430997916726a659ac56", "5d8c6990d02f478b8cbe0d69194ef67c", "b2836d5eeec04472a043e19722b7d608", "e39961291de240699244d4f01d58c885", "e2a36717bd4f4e6e8c8bf0ca056f2b7a"]}, "id": "dG9oPPuCTNaE", "outputId": "1b297d90-d407-4fb5-924e-4efa6368f8d0"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "03cbc655e874456094c25b88e56454df", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "cc1857d257024b129f7c1d5b24ebdd8c", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "5664699e31b24365b415d5f0409dac0b", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "b9deb9f0852346a0aef7ed3bb9c228d6", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "46b36bd52f2443369fbd33aa4e3e9f9f", "version_major": 2, "version_minor": 0}, "text/plain": ["merges.txt: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "4a0ee125e7724a048e82f115a13c395a", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n"]}], "source": ["# Create a zero-shot classification pipeline\n", "classifier = pipeline(\"zero-shot-classification\")"]}, {"cell_type": "markdown", "metadata": {"id": "hWysXOH0TNaE"}, "source": ["### Classify a Sample Text\n", "\n", "We will classify the sample text into one of the candidate labels provided.\n"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "NWRTeqebTNaF", "outputId": "e1fe5873-96ef-40f9-db91-e7017c24874e"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["{'sequence': 'This is a short course about the Transformers library', 'labels': ['education', 'business', 'politics'], 'scores': [0.7481651902198792, 0.17828458547592163, 0.07355019450187683]}\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Initialiser le pipeline z\u00e9ro-shot\n", "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n", "\n", "# Exemple de texte \u00e0 classifier\n", "text = \"This is a short course about the Transformers library\"\n", "\n", "# Liste de labels candidats\n", "candidate_labels = [\"education\", \"politics\", \"business\"]\n", "\n", "# Classification z\u00e9ro-shot\n", "result = zero_shot_classifier(text, candidate_labels=candidate_labels)\n", "\n", "# Afficher le r\u00e9sultat\n", "print(result)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "vPxabxJqTNaF"}, "outputs": [], "source": [">>\n", "{\n", "  'sequence': 'This is a short course about the Transformers library',  # The input text\n", "  'labels': ['education', 'business', 'politics'],  # The candidate labels\n", "  'scores': [0.7481651902198792, 0.17828458547592163, 0.07355019450187683]  # The confidence scores for each label\n", "}"]}, {"cell_type": "markdown", "metadata": {"id": "Q_21l5A_TNaF"}, "source": ["### Explanation of Results\n", "\n", "The output is a dictionary containing the input sequence, the list of candidate labels, and the corresponding scores.\n", "The scores represent the model's confidence in each label.\n", "\n", "In this example, the model has determined that \"education\" is the most appropriate label for the input text, with a high confidence score.\n"]}, {"cell_type": "markdown", "metadata": {"id": "y2SVij4bTNaG"}, "source": ["### Further Exploration\n", "\n", "You can experiment with different texts and sets of candidate labels to see how the model performs.\n", "Try classifying the following texts:\n", "\n", "1. \"The stock market is showing signs of recovery after a steep decline.\"\n", "2. \"The new policy aims to improve healthcare accessibility for all citizens.\"\n", "\n", "Use candidate labels such as `[\"finance\", \"healthcare\", \"politics\"]`."]}, {"cell_type": "code", "execution_count": 12, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "XjL3zvHdTNaG", "outputId": "0a9be0ce-5f4e-43b4-d976-32b6befba35d"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Texte: The stock market is showing signs of recovery after a steep decline.\n", "Classification: {'sequence': 'The stock market is showing signs of recovery after a steep decline.', 'labels': ['finance', 'healthcare', 'politics'], 'scores': [0.9854428172111511, 0.007386598736047745, 0.007170593831688166]}\n", "\n", "Texte: The new policy aims to improve healthcare accessibility for all citizens.\n", "Classification: {'sequence': 'The new policy aims to improve healthcare accessibility for all citizens.', 'labels': ['healthcare', 'politics', 'finance'], 'scores': [0.9620512127876282, 0.027672862634062767, 0.010275942273437977]}\n", "\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Initialiser le pipeline z\u00e9ro-shot\n", "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n", "\n", "# Liste de textes \u00e0 classifier\n", "texts = [\n", "    \"The stock market is showing signs of recovery after a steep decline.\",  # Exemple 1\n", "    \"The new policy aims to improve healthcare accessibility for all citizens.\"  # Exemple 2\n", "]\n", "\n", "# Liste de labels candidats\n", "candidate_labels = [\"finance\", \"healthcare\", \"politics\"]\n", "\n", "# Classification z\u00e9ro-shot pour chaque texte\n", "for txt in texts:\n", "    result = zero_shot_classifier(txt, candidate_labels=candidate_labels)\n", "    print(f\"Texte: {txt}\")\n", "    print(f\"Classification: {result}\\n\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "dOHoRNZpTNaG"}, "outputs": [], "source": [">>\n", "Text: The stock market is showing signs of recovery after a steep decline.\n", "Classification: {'sequence': 'The stock market is showing signs of recovery after a steep decline.', 'labels': ['finance', 'healthcare', 'politics'], 'scores': [0.9854428172111511, 0.007386600133031607, 0.007170595694333315]}\n", "\n", "Text: The new policy aims to improve healthcare accessibility for all citizens.\n", "Classification: {'sequence': 'The new policy aims to improve healthcare accessibility for all citizens.', 'labels': ['healthcare', 'politics', 'finance'], 'scores': [0.9620512127876282, 0.027672864496707916, 0.010275940410792828]}"]}, {"cell_type": "markdown", "metadata": {"id": "mYzJ5yXaTNaH"}, "source": ["### Control\n", "\n", " - candidate_labels"]}, {"cell_type": "markdown", "metadata": {"id": "pO1XWwAGTNaH"}, "source": ["## Text generation\n", "\n", "The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text.\n", "\n", "Here, we create a text generation pipeline by calling pipeline with the argument \"text-generation\". This pipeline will use a pre-trained model to generate text based on a given prompt."]}, {"cell_type": "code", "execution_count": 13, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 296, "referenced_widgets": ["5a42ee021bd048be9161c46d37fd32aa", "e6248c56ac394e6590fbd183f06643cf", "b35461d2e9434e76be84181da8349e6e", "235f917ba244430ca0f8880e1ee5bf76", "c093b8d9c41441c69805db0840a08748", "9531d256977b43a8b174f577af34cad9", "ed0bc9d9cdb14dd984d3dd0217b3fe9b", "e5a85a2a2b254331bbb39dc77990ef7d", "d7853013c92a4c6baff0722f4057bb4b", "bcdc52bd84db486393abdad6c02fbeab", "9b25b8a4bdcf46c1861aebb29748387d", "f971d2d8af2045f5b592953b1dc29aab", "70e8a6347f224c76820d110d205dee7b", "45cc1b0f172a46f1bd4ba6d3b9e98440", "70db66474672420f8a526584dc04a6e7", "75e063d19b00405891124616c672c1f1", "b9cec0059ebe4aa9891a6d19f7ec6984", "4b593a0112534818aff08069a5ef3be1", "9a7c050bf6924e32957a6cd5a182b16a", "bc77ae0cd4144e09be85f31bc4a819c3", "8e5cc411bc1547b2800efb14e1738ca6", "685030a00fd54bbc8c92082554b797cf", "8896b3cae53641f2865b54677bc16991", "64fede858be0439cab9166d862c9ab93", "50a12cdab9e24fb5b42af694c9a7d635", "f95d0b67075b4a958b54bd8d26926db3", "002b24759d734545b5f9a78440a2e350", "6b0fdcd7062e428da0ce6e277111490c", "cf8de9b976934ebfb6c98e4c93af7b01", "22822d888a414ba883383dd9b23b12ee", "8dc41329a88b459cadf522c469afb680", "9a19f34b683a4c39a1d6b867a6bdf139", "b68cecd090584ef7ac9a52cb36a3e5aa", "2a6b3abd8a0347a8adb5f349762c6799", "2c034fa0831944119d6b3fc59dcf1b71", "f79661af6abe48a4b87ca203c1eabce2", "4e9d385a1ada46f6a3fcb0a36b9601fd", "3634899f434247ec9026640b90372403", "0bc66cf6f1654468915a56b41ded0d35", "c3498f93af7b4a3abe47d2748657e85b", "1159986a42ee4455a94081a9321c8b20", "8fe5b8b007974ce988da60a9c44c3527", "160bbcb8ac614656b18c700bbbddc9f7", "c6685ac26f544ba2b02a3fd876a25b9c", "3dd2cea0bce444d2ab7d6f6bdb842f7b", "c57c7e7579154dafb3f01d3c874f2ce0", "26cdfc72be884751a7fb9ec829241c06", "f05ae75aa6d344dfb5e0d51c5e6b0a97", "e4167b09f28344ea8ddc3bc77302cd83", "d9a1e3ffbccd4a8f84b52dd16bd4c224", "eae6dc040b4342cca5a3e4ab0089f279", "44f73474b82c427cb3bec90d73f12153", "30b1c711638b4b0fa04362c8728c5543", "3606f92be7084c38989e640a19da93f2", "7144c57d64f84567b8eff2d967f78f7f", "cca106c8f429458e9e8883fa8d01f4bc", "7c7fb81998ac4907b100d51367bb66ee", "113e747239c84bdcb4563df3cff99101", "c97611c6c7f34e83a28dd4e52a25ee00", "6a003998367f458ebc6de4f95a7186ff", "d7ca555464fb4eaab9fe10b4bc7d883e", "0887a0e89781404c80cba46d31fd6332", "04739063bd524df5b25d0fa1bace6269", "ed7531330fe749c49280e291938d60b3", "1d5720e0f7ca4ba392a4f58ecaa463cb", "ffaf182c102d4d2fb73c301ba196b60c", "6acf755a73f74eae81128ac1e8510764", "4141150f72b8404fab6bd4e86fd63004", "3df507d9503c4fc8abb06d0a72c6ae04", "9c2997c2584e4ce8927d7bb23b35333c", "ac8a41eedfa140beb6e0e8c1e198acad", "1d84dcea0ac1430e8df23ff6281f03ce", "0f5da7881bcf43e0af75ba7338f577b8", "0460b85b54e24caeb936335ed6c203be", "e2143e5cd6024706a794cb429ef73cb7", "fa18bdfbf4254404bf5a150ca9faa41c", "66a282c5e2c24d46816424a15dfd5967"]}, "id": "yhidF-NSTNaH", "outputId": "5659d241-491e-4493-da3f-832f21b492e3"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "5a42ee021bd048be9161c46d37fd32aa", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "f971d2d8af2045f5b592953b1dc29aab", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "8896b3cae53641f2865b54677bc16991", "version_major": 2, "version_minor": 0}, "text/plain": ["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "2a6b3abd8a0347a8adb5f349762c6799", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "3dd2cea0bce444d2ab7d6f6bdb842f7b", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "cca106c8f429458e9e8883fa8d01f4bc", "version_major": 2, "version_minor": 0}, "text/plain": ["merges.txt: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "6acf755a73f74eae81128ac1e8510764", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n"]}], "source": ["# Creating a text generation pipeline using a pre-trained model\n", "generator = pipeline(\"text-generation\")"]}, {"cell_type": "markdown", "metadata": {"id": "74dP9JxHTNaH"}, "source": ["First, we define the prompt that we want to complete using the model. In this case, the prompt is \"In this course, we will teach you how to\".\n", "\n", "Then, we generate the text by calling the generator with the prompt. We also specify max_length=50 to limit the total length of the output text to 50 tokens, and num_return_sequences=3 to generate three different sequences based on the prompt.\n", "\n", "Finally, we display the generated text"]}, {"cell_type": "code", "execution_count": 14, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000, "referenced_widgets": ["4dd52615f0f24a29badbe4f497cb8ea7", "988b665a8c31488ab99561ef41900ffe", "b5447625e0ee42cc967c6e4bddd07ac3", "6e596a65984941daa0580ff503f451ce", "8133b016cd3a48aabc9e57a1740a622b", "05093955a7174e30a2db081e74def450", "977c48aa825e4e8d9da78b99a8e77e8b", "c1b969d2f39c4031b8277907beb77f40", "f94c095139c44630bf0d3c25b27f2dea", "87c3145b625f45bcaadc6c5e418ab8ae", "94ec2c54a04648c7b6e0574e98f5d9d8", "43f2f182c74343f986c8cc731455eb63", "7c62330d01b346019afcb991fd9c2afa", "78e5ba924086472fa8fb0f6ded1640f2", "2e2a1e1353864eb18d5d9421a18d0e64", "99bc92badc9242b5a65be8fb2f59b2ab", "a623ba2760174d3dbf89ac45a1263d52", "0093d34d59ff4b7183acbba0d9e31281", "4ce49db6f45d47e2bfb633896eba43f0", "4f88bbf2cc724a4ca91ed195b4aad697", "7baa338c04fe42c18d0c4e0303d7c494", "e5a43b0bab574af2a67fcc32204c4845", "7b8d4472cc404e2db7c5b50f4c8829ac", "67ef10944d0640ecbb7433a90fc05655", "a74a11d34b3745a8be90725dc206ac0a", "cbed7c32007f4c079dcad75b1ac2769f", "aa57973192f7473aa62647925fd3e6a1", "efecc93104c54242a791a8d6b2b519b3", "745dd51e648740d6926abeb31e9cffdc", "ab0c7ec6368f425ea524ed234b79aa96", "0de52c096c0947c0a636a855b83a130f", "e41a56305a854dbd838b335cecba845a", "bba1eab4dde145f0bc0a7128ebe12b9c", "de7ea364d1434bed8a0859b1e18a6422", "6deb32849c79415998c0736b070f61d8", "3cf9c2d977034e88afb29009210b203b", "2c98ba764b264f51b870c3d4bd072395", "32018dadf9f945b79b2a23b40b09fe03", "11413b36cff84edfa74143960c1deed9", "3780013fc8b34cdfaafae7114aad65c2", "44c609640d9e4e959d0bb3700c4079c8", "4a3cfdd12e3d4cd6a9b4d44be26402c1", "3658ff3299e4497493142027f4ecd011", "c90df9577fe1497980ed23a123a5b37d", "b8769d6cc8b940d28160790d5cc5a553", "28d0cffa210547e58e8f322696445aeb", "5e7cea8502814e3e9e21c34ee991444e", "f5338a9cb6ac4cea9d2ae70a09858591", "0b5f6d7b50884423a7f8b1605188dcba", "c269b5b74e3b4184aadaf908ea1db338", "b91903321e6f47ee8ba27e5f7b99b2f3", "db7aa0de940b4269853434d4692ae79b", "fd146f7d06df4ef6811d5e347f7e5739", "2a6921e8ee9a4c5e810ea520da7a2e2a", "99d14579e48d41e5b0d6648e7db4141e", "726b20de11374fba9c0d078497e95d9b", "550d423a51fc41069c4a6b431f3347af", "8fed0a18677241efbbf7aadf2e0af540", "78be22d0eedc4e9990634b74ac83d08b", "eee9c07a9d7645deb69f771fd93e284e", "687c712d1b7140b1a1aed7815327103d", "142d599cd9a440ad8a093233bb6e2601", "49bd52e1cb044b67b51828b2e2e29db7", "21d743a11ad946ac858802ff58af70e2", "649fa16faebf4cd296b7c87709fced70", "722e0c06acf54924a2e774bd0a2ab428", "aebcd85d8c5c4a61b8bae4c767aea861", "d0d626abd5114eb09c1f75366fe0bb6c", "47cd27ad74e04e41addaeb227fac2dc8", "07c1d603b7214cd290b3aab61e9dfb17", "2b354cc3611946f9aeb0334bdde383b3", "0b5a223d392b42419cf5c287c6851723", "4b717370c27e43058206930ff38e29f1", "abe87bbb21a0482298bcd3a81848944e", "ccaa346e83764eeaaceb642da39075f0", "7d1fea5e77d74bd0a078537914639805", "2ce438abe849456f8008d814865bed4a"]}, "id": "PSo3E0_iTNaI", "outputId": "96c3c55d-0447-456d-b50d-7293d142a0b3"}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "4dd52615f0f24a29badbe4f497cb8ea7", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "43f2f182c74343f986c8cc731455eb63", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "7b8d4472cc404e2db7c5b50f4c8829ac", "version_major": 2, "version_minor": 0}, "text/plain": ["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "de7ea364d1434bed8a0859b1e18a6422", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "b8769d6cc8b940d28160790d5cc5a553", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "726b20de11374fba9c0d078497e95d9b", "version_major": 2, "version_minor": 0}, "text/plain": ["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "aebcd85d8c5c4a61b8bae4c767aea861", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n", "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n", "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n", "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Generated Text 1:\n", "In this course, we will teach you how to set up data-driven data analysis tools that will help you better understand your data, what you need to do to build them, and how to use them to analyze your business.\n", "\n", "This course is designed to prepare you for the next generation of data analysis, and will give you the tools and tools necessary to start using these tools and tools, while also helping you to work on your data.\n", "\n", "This course will also provide you with practical advice on how to get started, and how to plan your data for your business.\n", "\n", "This course is designed to be an introductory course, to help students learn about data analysis, and to give you the tools needed to start using these tools. This course is designed to help you to quickly build your data, while also help you to develop your data so that it can be used for your business.\n", "\n", "This course is designed to be a introductory course, to help students learn about data analysis, and to give you the tools needed to start using these tools. This course is designed to help you to quickly build your data, while also help you to develop your data so that it can be used for your business.\n", "\n", "This course is designed to be a introductory course, to help students learn about data analysis, and\n", "\n", "Generated Text 2:\n", "In this course, we will teach you how to build an app that can be used to help people navigate the world without any real assistance.\n", "\n", "It's a pretty basic thing, but if you'd like to see the full course and why it's important for all Android developers, check out the full course on Github\n", "\n", "You can find a lot of resources on the Google Play Store for the course on how to build the app, but don't miss out on the opportunity to get your hands on it via the Android app store or Google Play Store.\n", "\n", "The course will be taught by Chris Daley, a developer at Moxie, a startup that offers social networking services.\n", "\n", "The course is available on both Android and iOS.\n", "\n", "Generated Text 3:\n", "In this course, we will teach you how to understand how to use your own computer to make a smart phone call, which can be done in minutes.\n", "\n", "We will explain how to use the Internet to make a smart phone call, which can be done in minutes. We will also explain how to use your smartphone to make a smart phone call, which can be done in minutes.\n", "\n", "Our goal with this course is to teach you how to use your own computer to make a smart phone call, which can be done in minutes. We will also explain how to use your smartphone to make a smart phone call, which can be done in minutes.\n", "\n", "We will also explain how to use your computer to make a smart phone call, which can be done in minutes. We will also explain how to use your smartphone to make a smart phone call, which can be done in minutes.\n", "\n", "How to use your own computer or smartphone to make a smart phone call\n", "\n", "How to use your own computer or smartphone to make a smart phone call\n", "\n", "How to use your own computer or smartphone to make a smart phone call\n", "\n", "We will also show you how to use your own computer or smartphone to make a smart phone call, which can be done in minutes.\n", "\n", "We will also show you how to\n", "\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Initialiser le pipeline de g\u00e9n\u00e9ration de texte\n", "text_generator = pipeline(\"text-generation\", model=\"gpt2\")  # On peut choisir \"gpt2\" ou un autre mod\u00e8le\n", "\n", "# D\u00e9finir le prompt\n", "prompt = \"In this course, we will teach you how to\"\n", "\n", "# G\u00e9n\u00e9rer plusieurs textes \u00e0 partir du prompt\n", "generated_texts = text_generator(prompt, max_length=50, num_return_sequences=3)\n", "\n", "# Afficher les r\u00e9sultats\n", "for i, gen in enumerate(generated_texts, 1):\n", "    print(f\"Generated Text {i}:\\n{gen['generated_text']}\\n\")\n"]}, {"cell_type": "code", "execution_count": 15, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "yUg1cY7GTNaI", "outputId": "a0e7bf04-4d8d-42f5-9d0e-8ef0af72c319"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Generated Text 1:\n", "In this course, we will teach you how to set up data-driven data analysis tools that will help you better understand your data, what you need to do to build them, and how to use them to analyze your business.\n", "\n", "This course is designed to prepare you for the next generation of data analysis, and will give you the tools and tools necessary to start using these tools and tools, while also helping you to work on your data.\n", "\n", "This course will also provide you with practical advice on how to get started, and how to plan your data for your business.\n", "\n", "This course is designed to be an introductory course, to help students learn about data analysis, and to give you the tools needed to start using these tools. This course is designed to help you to quickly build your data, while also help you to develop your data so that it can be used for your business.\n", "\n", "This course is designed to be a introductory course, to help students learn about data analysis, and to give you the tools needed to start using these tools. This course is designed to help you to quickly build your data, while also help you to develop your data so that it can be used for your business.\n", "\n", "This course is designed to be a introductory course, to help students learn about data analysis, and\n", "\n", "Generated Text 2:\n", "In this course, we will teach you how to build an app that can be used to help people navigate the world without any real assistance.\n", "\n", "It's a pretty basic thing, but if you'd like to see the full course and why it's important for all Android developers, check out the full course on Github\n", "\n", "You can find a lot of resources on the Google Play Store for the course on how to build the app, but don't miss out on the opportunity to get your hands on it via the Android app store or Google Play Store.\n", "\n", "The course will be taught by Chris Daley, a developer at Moxie, a startup that offers social networking services.\n", "\n", "The course is available on both Android and iOS.\n", "\n", "Generated Text 3:\n", "In this course, we will teach you how to understand how to use your own computer to make a smart phone call, which can be done in minutes.\n", "\n", "We will explain how to use the Internet to make a smart phone call, which can be done in minutes. We will also explain how to use your smartphone to make a smart phone call, which can be done in minutes.\n", "\n", "Our goal with this course is to teach you how to use your own computer to make a smart phone call, which can be done in minutes. We will also explain how to use your smartphone to make a smart phone call, which can be done in minutes.\n", "\n", "We will also explain how to use your computer to make a smart phone call, which can be done in minutes. We will also explain how to use your smartphone to make a smart phone call, which can be done in minutes.\n", "\n", "How to use your own computer or smartphone to make a smart phone call\n", "\n", "How to use your own computer or smartphone to make a smart phone call\n", "\n", "How to use your own computer or smartphone to make a smart phone call\n", "\n", "We will also show you how to use your own computer or smartphone to make a smart phone call, which can be done in minutes.\n", "\n", "We will also show you how to\n", "\n"]}], "source": ["# Afficher les textes g\u00e9n\u00e9r\u00e9s\n", "for i, gen in enumerate(generated_texts, 1):\n", "    print(f\"Generated Text {i}:\\n{gen['generated_text']}\\n\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "4vHicPV8TNaJ"}, "source": ["### Control\n", "\n", " - max_length: total length of the output text.\n", " - num_return_sequences: number of returning sequences."]}, {"cell_type": "markdown", "metadata": {"id": "KBvs5aUaTNaJ"}, "source": ["## Using any model from the Hub in a pipeline\n", "\n", "The previous examples used the default model for the task at hand, but you can also choose a particular model from the Hub to use in a pipeline for a specific task \u2014 say, text generation.\n", "\n", "Let\u2019s try the distilgpt2 model :"]}, {"cell_type": "code", "execution_count": 16, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 481, "referenced_widgets": ["59d8ff176f5e445a99ff3bf9da5a32f4", "6adc355dd075439297723d7d8a7b2c5e", "0785cb0ad42143ee9f96eea0593c44a9", "3746498420e746439f24e5c380c58f22", "a86234e9d0014ad2b8a139ff70c0cbb6", "bc4425d7e19744c4b28a7097da1ba6f1", "6db7f6cab378418897f32c061b088a68", "4a3c2242a0904bd98306bd923b7b1053", "43c48b65e8a445d8a47209be45d8b6a2", "7bed9ca4aa464c3e82ce0fbd01528976", "f186a5e948c741f5aa8b121ef8b1d3d7", "21b10870933c40f1854c3235a6c72397", "ff6849f83dac4ec981cf1d3d77afd3d1", "9daef18d33b34ebebd9a777a63895a31", "0e5833c8563e48579cabd4ea3f16d854", "3218e111f68443d6a1bd3956cbe9003d", "825333320eb44d6fbafcbf65a895ccb0", "a15e307424a94d6bbf60fc6ee69bd673", "84438c7c0db7436db707474a7079f443", "7d245037e21345399522dcaa3a0216d7", "75b54b5d938e4ab997726f44671dfd61", "13eb87a78609429d8d473459064a1bba", "13d7fcc395324c798b73b56137932156", "173890aeb6f9411fbb72c4fd142f0e2c", "e1c18a50b69444e9a1a96bc7f24a4eeb", "e4337c6f27964a2c89afaa1796508b25", "deac36d60ac3479f8951a6ceff7e762d", "c7a12045b7664867b998617aa2ba3b96", "87b60236c5594e8c8538911bfc74dd73", "e3d99e71805245878679e7d81ccc4054", "5fb635ae5a0240b9ac4c41085886469c", "9c41e15aa24041388bee049fbcdd1d52", "e0cad038790d4a7aaf07c62468b4ac48", "a95e474864bf4ba4afa51aecd1c0ca53", "36e4d6edfa9345ae9351c6f8756deb28", "6752926ecd1d40e999fc2c68289b2f23", "c5acede2093e4d5e9ed0bdc8d252d593", "cbadb3b7709440989d1bd9100fd158dd", "5df6db67b42944a6ae9612cc3aaf9501", "6715f6274d7245b5b5fff84d6f78a8f4", "63daedc075df43c1bd37664b9a2c4ae0", "ff9133ce1c844e6e9a26012d89a79692", "b5e4434b7c8e41d1820c7d64899d2ff2", "e5d51c023339429cbdf7452491fdbab9", "b54a0c3b6255438b85c09eeaa31c1626", "1c32a57da4814649aabde0be81bf0f4e", "ebaba341870049d4bf07f2f50aca45b4", "e9bd52c466414ca1946700ae8dd25ff1", "885f387748e2447cb35df1e1d72141bd", "95bdbbaa1d57488b9aa4ac129c634bc0", "1e1dc406638848fe9facbda7770ffd6b", "401db3ae9eb147b784682166afffb885", "9f6ec15e137c423e854e2b08ee9a3aca", "06bd10b4bf894dcf947c3252f58b551a", "e214372ddb5c4a4a9d93d0d23755e624", "676d467c0d6242b4b81ce741d878f6f9", "cc0c6b5ddbf9428487ccb9ad25a10f56", "feef1358e61e4a908ecf51a08502c8eb", "8025f015f7cc40d7850e184479dbf5d9", "c4c50a0f5e5d40d6b517e643e20bf578", "f0eca91d4e514bbfaed7200637e2135d", "185cb70897e84366995a1f721b999dee", "97dce68186444fbc9f0e1b25868ddd84", "5b9ec784c90a4db1a5aebc1cef1d40ea", "982accb22ec048a4bc7d689c17b4d0eb", "e30cd98eb9a24e31979cfa8ed4f51c24", "5fd446f0b94c4197b61e1125a311ebac", "65836034a910450ebd685a08044ee3b6", "100836405ab041ac84d23e484dfca182", "36240824779c48c7bf0d6af431e32d20", "573c003c633d41b0943a04e08007102b", "b2bdaf30e0624eaab3ffec6781223be0", "ba178904847e4fcdb2e532b894eaa92b", "6dd4931cf8574388819ded6377bb3791", "51eea6d8e6e1419e946f3a3074afd818", "8d698e46bfbb47e19e14853a93ad804a", "efb3e53046f04d248a7cc2d2968477f8"]}, "id": "iPy46ndBTNaK", "outputId": "72ba1968-3c91-42ac-96e1-6631e95b3fb8"}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "59d8ff176f5e445a99ff3bf9da5a32f4", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "21b10870933c40f1854c3235a6c72397", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "13d7fcc395324c798b73b56137932156", "version_major": 2, "version_minor": 0}, "text/plain": ["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "a95e474864bf4ba4afa51aecd1c0ca53", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "b54a0c3b6255438b85c09eeaa31c1626", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "676d467c0d6242b4b81ce741d878f6f9", "version_major": 2, "version_minor": 0}, "text/plain": ["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "5fd446f0b94c4197b61e1125a311ebac", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n", "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n", "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n", "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Generated Text 1: In this course, we will teach you how to build a successful business model that will help make it happen.\n", "\n", "\n", "\n", "Awards\n", "\n", "Awards\n", "Generated Text 2: In this course, we will teach you how to use the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of the language of\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er un pipeline de g\u00e9n\u00e9ration de texte avec le mod\u00e8le 'distilgpt2'\n", "text_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n", "\n", "# D\u00e9finir le prompt\n", "prompt = \"In this course, we will teach you how to\"\n", "\n", "# G\u00e9n\u00e9rer du texte avec les m\u00eames param\u00e8tres\n", "generated_texts = text_generator(prompt, max_length=30, num_return_sequences=2)\n", "\n", "# Afficher les textes g\u00e9n\u00e9r\u00e9s\n", "for idx, gen in enumerate(generated_texts, 1):\n", "    print(f\"Generated Text {idx}: {gen['generated_text']}\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "Y71wzVJmTNaL"}, "source": ["## Mask filling\n", "\n", "The idea of this task is to fill in the blanks in a given text using pre-trained language models.\n", "\n", "### Creating the Unmasker:\n", "\n", "This line initializes a pipeline for the fill-mask task. The \"fill-mask\" argument specifies that we want to use a model trained to predict missing words in a sentence."]}, {"cell_type": "code", "execution_count": 17, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 339, "referenced_widgets": ["81ed96448e034d25a0e4b760dc3863da", "a5a5811e8d084535b202d2b9c8af58b7", "8c3c26ad4768427b8f1157eb8e81494a", "299cf61ed5cd4c4f9373797400eca2e2", "1780d150363d46fd8855a7e4c15afe38", "0f5064bb5bda4c3aa4bd7561940f1ec8", "b37c068cb89240f2a8672e467595243f", "00bd9d2b6e7e43a0b67d8d143b430a33", "ec824bfafb4a40e6a4cfa88ee69a6b51", "daffc895e12e46159559d3b7531e6aac", "0a8856e8f3094d0a8f3857ca9837e145", "5910094ab9054d4a88a16cd78c797349", "605e6de2c9e84b88b338256dfa6fb4cc", "758e642ec0284cddb5e916e30d65e987", "0db9c6d6be0b433f9f15d2730e132891", "4e9cc38112094546987be0000ed0c2c2", "a2a9644e16a648bcb902308cc6e57dc7", "2704cf5fb39d4f4483815cfe52f0da5d", "3745687d04744d9199d7ed82786aace2", "f300554dd7fc4eb3bb5d0a1ef453a669", "cf234dfca9c3447da8d6db6edac23625", "2d0738bf739e4ea7a0b05ee60170a899", "f4c5f87a60184716a1dcae6e54e76652", "cab7ad1ba9f041fd89ab7530a8e36ef2", "669ed7d984f746878afc7c9dbe4665d4", "94809479873e4b6297bf32908cd9cc32", "c2949e5e261e49c8965caf583298454f", "39743bf3b7c64331862655a7c253bf82", "c9fcd80ade50443a903ecf6fe1b2f1c9", "ea6afaf037e14f59b5bc3b2ff3cc46e5", "ed57bfed54c44b689d64c23231771046", "e41df59c3dfc4d2297b18279d579c402", "bb38c6f0425146d0a93589c831b3f225", "10a8ca326b27476c86fbf38e8d11d57d", "faf35909ca754510924520e9117deef6", "aceff8ba637143e49933fca9ff3d30ff", "65f57a6525c144da8d3f70f5f995f3f1", "99f51985ec9d49768dcc9651f26a24b5", "a3bf9ec45adc4d33ad315ccf0ede6b4e", "58e12efe0b0247a6ba3442b3bc006d62", "ebee91828b3549c5b91383262123b59e", "177ae1f4863d445b8b5c5bd7072e5ee5", "4cbda172b6884ae0bfa9badcb67f1226", "9cf6a29e7943412aa582c144b0939546", "1f128a048d2448868899b377b40d085d", "24798827b90b47dd8dc2679a12106964", "5dabdfe4fdbd44d8aaf8ca18e7b25461", "fbb13b18c019437b9568444ef7bd543a", "5b4bd71223c64e6f8fd12e76b6e0995a", "9670b2f9a4d1452eadf48d91ede653d1", "12762e09ae9b45db93c1b706f6f05fd2", "c1ec89a2c5de4013bd27fabe488b42df", "b9376f28381b4cf5845b949ac5bb0d88", "1d44a0a94aba4d0fb7d89b672190fdf2", "471bb0debbfe48feb754e72c89925e2d", "1a6e5079df8b49d4899c6a798c602782", "26d2809940b643d5951a4a053d1de970", "6ab6fa1c7dde41c6833fe8e07ff30486", "d07771947acc477bb04175de4da3b2c2", "a93fcccef88f4701930dcf07aa70054e", "df4fcda4b4a54aa5a7a2f79466c543bb", "510ac626b74d402191c7f10913df51e2", "acf216a64daf498080794f434f59e097", "d3876662e2b34e388722e80b64bf2529", "759bf8e8616349fe8a11472c3a5873c8", "e707dbf714aa46938222d893ed45e66a"]}, "id": "8FT5q8fvTNaU", "outputId": "fc782c6f-2e57-4bb9-efdc-ddb774b09760"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "81ed96448e034d25a0e4b760dc3863da", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "5910094ab9054d4a88a16cd78c797349", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n", "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n", "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "f4c5f87a60184716a1dcae6e54e76652", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "10a8ca326b27476c86fbf38e8d11d57d", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "1f128a048d2448868899b377b40d085d", "version_major": 2, "version_minor": 0}, "text/plain": ["merges.txt: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "1a6e5079df8b49d4899c6a798c602782", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n"]}], "source": ["# Create a pipeline for the fill-mask task\n", "unmasker = pipeline(\"fill-mask\")"]}, {"cell_type": "markdown", "metadata": {"id": "tTiRMFmXTNaU"}, "source": ["### Filling the Mask:\n", "\n", "Here, we use the unmasker to predict the masked word in the sentence. The top_k argument specifies how many of the top predictions we want to display. Here, we request the top 2 predictions."]}, {"cell_type": "code", "execution_count": 18, "metadata": {"id": "285oFMSqTNaV"}, "outputs": [], "source": ["# The '<mask>' token is a placeholder for the word that the model will predict\n", "results = unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"]}, {"cell_type": "code", "execution_count": 19, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ScJGu5poTNaV", "outputId": "f263b04a-e1c6-4a03-cd85-5ffe317e8fd9"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'score': 0.19619767367839813, 'token': 30412, 'token_str': ' mathematical', 'sequence': 'This course will teach you all about mathematical models.'}\n", "{'score': 0.04052715748548508, 'token': 38163, 'token_str': ' computational', 'sequence': 'This course will teach you all about computational models.'}\n"]}], "source": ["# Display the results\n", "# The results show the top_k predictions the model suggests for the masked word\n", "for result in results:\n", "    print(result)"]}, {"cell_type": "code", "execution_count": 20, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Vg2Gk1ptTNaV", "outputId": "6a5bd132-b0a4-4c79-d930-dec9af7c66c1"}, "outputs": [{"data": {"text/plain": ["{'score': 0.04052729159593582,\n", " 'token': 38163,\n", " 'token_str': ' computational',\n", " 'sequence': 'This course will teach you all about computational models.'}"]}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": ["{'score': 0.19619794189929962, 'token': 30412, 'token_str': ' mathematical', 'sequence': 'This course will teach you all about mathematical models.'}\n", "{'score': 0.04052729159593582, 'token': 38163, 'token_str': ' computational', 'sequence': 'This course will teach you all about computational models.'}"]}, {"cell_type": "markdown", "metadata": {"id": "eGrcX-55TNaW"}, "source": ["This loop prints out each prediction result. Each result includes:\n", "\n", "score: The model's confidence in the prediction.\n", "token: The token ID of the predicted word.\n", "token_str: The predicted word.\n", "sequence: The full sentence with the predicted word filled in."]}, {"cell_type": "markdown", "metadata": {"id": "mo__PSIuTNaW"}, "source": ["### Control\n", " - top_k argument: controls how many possibilities you want to be displayed.\n", " - <mask>: mask token or special word the model must fills in. It depends on the used model."]}, {"cell_type": "markdown", "metadata": {"id": "4MwH4I1qTNaW"}, "source": ["## Named entity recognition (NER)\n", "\n", "NER is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations."]}, {"cell_type": "markdown", "metadata": {"id": "ok73YKGuTNaX"}, "source": ["### Creating the NER Pipeline:\n", "\n", "This line initializes a pipeline for the named entity recognition (NER) task. The \"ner\" argument specifies that we want to use a model trained for NER. The grouped_entities=True argument groups together consecutive tokens that are part of the same entity."]}, {"cell_type": "code", "execution_count": 21, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 349, "referenced_widgets": ["c6bcce8c13984ff282fbb5ef76c1327a", "c4c70c850c1c4ebab6527249a05864ae", "a91b5cd3f1d64e12b07b5b78fa7152e2", "be66a596efdb4680a9ab49977bd80116", "3bbb36336e9747ac803deb67f9e319a5", "3485d884cdbc4ca0af15079dedeb6328", "6ab9638b64244d25b404d60519045eb3", "534e4799a7b249bebf8e6c4009f7276c", "9971950260d447dfa44a5bfcdeaa8e32", "2beebbcffab34a3cb3daed5cf0e4e975", "a085a23a2bec4e709ea78354c2ff9542", "8adce99b914a4fa4b341d4be84faf79d", "a141400aeb814573a78c3b30331e56ca", "e09103636bc748408e51b50e697b916b", "71b9e4c19b9d42cf89af4202770139fc", "0491a0d42183447399143d0b76eae828", "284a74282d3845dbb1580fc144d3ab13", "4e666fb02463411c9bb01959d66878d1", "bacf1a75f7154beab90b8f009d72038b", "a24315968dee45a1ae7e026de981f3e4", "688aff162b4948cabededd8272f18d0f", "d10287b4172d4442a818d0bec3546258", "d94847072386460fb39ac7fdffa31494", "4021b8c7f37f4a5db88a15f6467ac198", "e11f47ab88614289b28cc136249a8f54", "c7c6781a8cec4332a2e876df3e627bc1", "4c501682616c4823b1215d94cf43c091", "9647d4afc1264fd4ae8b0ade62a17ff8", "face5cfde27a4d54b8d1189483325356", "8bfa146aba374fc1aaf8c94e35af3ef2", "407655973e1a4c2a818bd1de7d55786f", "c5ce2e1f3d3c44b2aefa9d2e75d6e872", "c59edcc1139743e8905e62df7ec9ca2e", "3bb24b0a871d43648b95fa8cf8d50a98", "565cb9c1883f4e0ab10692e6776298ce", "30f0f1e0d84c41709f39d87adebbc0d5", "682e548e0a0147119b0231b3b2d8b3bb", "e3e7708f3d654c9faaf39e6a0524552f", "02448324e43141e6929928812e23aa32", "5b0f7a091c6a455283fc3f8d286bfa0c", "12c68575088d4914948f6edbd8a2eac7", "6f4abd9a1ef749819a310aade2db433e", "1a51f2f1ffa144bb82002d16e4f22654", "f83af958ee06441f97d0a37d2d9e768d"]}, "id": "kRJpUfVTTNaX", "outputId": "9aa313b2-b620-442f-d7b7-31867e352018"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c6bcce8c13984ff282fbb5ef76c1327a", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "8adce99b914a4fa4b341d4be84faf79d", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n", "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n", "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "d94847072386460fb39ac7fdffa31494", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "3bb24b0a871d43648b95fa8cf8d50a98", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.txt: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n", "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n", "  warnings.warn(\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Entity: Barack Obama, Type: PER, Score: 0.9992\n", "Entity: United States, Type: LOC, Score: 0.9987\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er un pipeline pour la reconnaissance d'entit\u00e9s nomm\u00e9es (NER)\n", "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n", "\n", "# Exemple de texte\n", "text = \"Barack Obama was the 44th president of the United States.\"\n", "\n", "# Appliquer le pipeline NER\n", "entities = ner_pipeline(text)\n", "\n", "# Afficher les entit\u00e9s d\u00e9tect\u00e9es\n", "for entity in entities:\n", "    print(f\"Entity: {entity['word']}, Type: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "C-Skib4ATNaX"}, "source": ["### Identifying Entities:\n", "\n", "In this line, we use the ner pipeline to identify entities in the given sentence."]}, {"cell_type": "code", "execution_count": 23, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "4kMzd2G0TNaY", "outputId": "04d8f97a-fe8d-4885-f8ca-a480dfcbd975"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n", "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n", "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n", "Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Entity: Sylvain, Type: PER, Score: 0.9982\n", "Entity: Hugging Face, Type: ORG, Score: 0.9796\n", "Entity: Brooklyn, Type: LOC, Score: 0.9932\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er le pipeline NER\n", "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n", "\n", "# Texte \u00e0 analyser\n", "text = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n", "\n", "# Identifier les entit\u00e9s dans le texte\n", "entities = ner_pipeline(text)\n", "\n", "# Afficher les r\u00e9sultats de mani\u00e8re claire\n", "for entity in entities:\n", "    print(f\"Entity: {entity['word']}, Type: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "1F02i29NTNaY"}, "outputs": [], "source": [">>\n", "{'entity_group': 'PER', 'score': 0.9981694}\n", "{'entity_group': 'ORG', 'score': 0.9796019}\n", "{'entity_group': 'LOC', 'score': 0.9932106}"]}, {"cell_type": "markdown", "metadata": {"id": "6bEBFemATNaY"}, "source": ["This loop prints out each identified entity. Each result includes:\n", "\n", "- entity_group: The type of entity (e.g., PER for person, ORG for organization, LOC for location).\n", "- score: The model's confidence in the prediction.\n", "- word: The entity found in the text.\n", "- start: The starting position of the entity in the text.\n", "- end: The ending position of the entity in the text.\n", "\n", "In the given example, the model correctly identified:\n", "\n", " - Sylvain as a person (PER),\n", " - Hugging Face as an organization (ORG),\n", " - Brooklyn as a location (LOC)."]}, {"cell_type": "markdown", "metadata": {"id": "eeqYkpsvTNaZ"}, "source": ["### Control\n", "\n", " - grouped_entities=True: regroup together the parts of the sentence that correspond to the same entity (grouping \u201cHugging\u201d and \u201cFace\u201d as a single organization)"]}, {"cell_type": "markdown", "metadata": {"id": "raDJVFb6TNaZ"}, "source": ["## Question answering\n", "\n", "The question-answering pipeline answers questions using information from a given context."]}, {"cell_type": "markdown", "metadata": {"id": "DoIxcv9sTNaZ"}, "source": ["### Creating the Question-Answering Pipeline\n", "\n", "This line initializes a pipeline for the question-answering task. The \"question-answering\" argument specifies that we want to use a model trained to answer questions based on a given context."]}, {"cell_type": "code", "execution_count": 24, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 232, "referenced_widgets": ["aed35777d8444604ba667498f7b9396a", "2bbca16dbfe546d488c44e7870cb54d3", "0b720a71b7dd497a9cceaac371ca63ad", "3423746340e44f8a80ae407644d7f3d5", "018710f4e93246f4a714d319bbc5f331", "ad28c8155fe24245afe3d049d84f1023", "749096d6aa8f46d7a801085653e1c988", "0ddb6ed89698469092a521da404417c6", "011f424e59074d4e8206c118d6f4ad57", "36c240f5d3854720b30c9c6b7bdc8387", "87152f4359b04a429139245337426f2d", "9450402b82574a9a92cae6f502ad7a97", "42257bcfb4494b5eb003e033dcf00554", "d73567776bda4d7ea41417da3011f8ba", "6bda15c16e434b4494b4538cd4d5b52a", "dcf32e175b404f38a78bb69f2293d552", "fb05e4f6df824a88a8bf648a50893332", "9e6838f8de404ab3aa17b39941e4f6d8", "595d4875e1b840cb856a5f80e9814449", "6f15fc5295ef42ffa0ee78977f7c9e8f", "3aa42ae369fa44a3b99281de31376f36", "23ace1a2d5154bd9bc612d24096170aa", "a62fee3a0fcd4da4a5a0890ba5fcad86", "3afcc354459c4d538438e25ce2924110", "15bcc8daa0314c5eb46e6bfbf1e26274", "83296d22c241428c942cd88c6764dcea", "3cb3a272c51b404f8c0a80ce20ca65fd", "0d19a1d1ff4345d8b08bc0e106f6ba32", "7cc499a0201e4c8c8eac116c088cc639", "4f208e40a07a4a8f8cb060b971c74cad", "a8de2190fc634657b0f8f8b5b9c4273e", "079d2b3d834a40e3b6411ac7d4945862", "4b46a58ec3634068b4bafc26731ca7ae", "a172e07f15c843b197cf2a745e29dde1", "8c37cbc6296c48099fdcd9fb7ba9511e", "c46a421c05d54444a270b0f0f4188e1d", "85b3d2d2563b47d68c8a42d2110bef73", "e772e13c610b408daeae562f7bd3336c", "707431989b444d8fad25c2966995a4f2", "4a49c2203bce4609a6e6ba993db07c0e", "f0b1cfa628834758b53c4e1187a16e04", "6a9c6af2334f4991869bc900e3520205", "4423ddfa5da8427f9f761a7669d53322", "1c1650cd6a6542beb33da7f14bac7d58", "3e3d1b4e40c14cff9c032d90c536f016", "b5aff4f48b8e4256a506b730a849b105", "cee42b59b1194796a17a7bc61507765e", "050188b3bc0d49b8be8b6820e04b035c", "37d858a62fc24eeb9f5fc9963ca567cd", "8e64b59e7eb9435d95c0c6aa918129c4", "91c19a65a10640cc82fa4b2dbff2597e", "0f3c9eb7e9f74b7db724ea9914b9281a", "e39100e4e8574511bb750df7401f3db2", "97cfb40e456b478a998e894d31963acd", "15ab133028ce491b8fd6c2c412ac225f"]}, "id": "Uw6lqK6WTNaa", "outputId": "7b3f0154-7f50-4efb-b80d-a2eaed56283b"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "aed35777d8444604ba667498f7b9396a", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "9450402b82574a9a92cae6f502ad7a97", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "a62fee3a0fcd4da4a5a0890ba5fcad86", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "a172e07f15c843b197cf2a745e29dde1", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.txt: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "3e3d1b4e40c14cff9c032d90c536f016", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n"]}], "source": ["# Create a pipeline for the question-answering task\n", "question_answerer = pipeline(\"question-answering\")"]}, {"cell_type": "markdown", "metadata": {"id": "y1GjEppwTNaa"}, "source": ["### Answering the Question\n", "\n", "In this block, we use the question_answerer pipeline to answer the question \"Where do I work?\" based on the provided context."]}, {"cell_type": "code", "execution_count": 27, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "lh2vhBGGTNaa", "outputId": "f543e62a-e755-47fe-87e1-ad414ee14fe9"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Question: Where do I work?\n", "Answer: Hugging Face\n", "Score: 0.6950\n", "Start: 33, End: 45,Answer: Hugging Face\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er le pipeline pour le question-answering\n", "question_answerer = pipeline(\"question-answering\")\n", "\n", "# Contexte et question\n", "context = \"My name is Sylvain and I work at Hugging Face in Brooklyn\"\n", "question = \"Where do I work?\"\n", "\n", "# Obtenir la r\u00e9ponse\n", "result = question_answerer(question=question, context=context)\n", "\n", "# Afficher le r\u00e9sultat de mani\u00e8re lisible\n", "print(f\"Question: {question}\")\n", "print(f\"Answer: {result['answer']}\")\n", "print(f\"Score: {result['score']:.4f}\")\n", "print(f\"Start: {result['start']}, End: {result['end']},Answer: {result['answer']}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "GVCxniE8TNab"}, "outputs": [], "source": [">>\n", "{'score': 0.690, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"]}, {"cell_type": "markdown", "metadata": {"id": "GZA7TQF9TNac"}, "source": ["This line prints out the result of the question-answering task. The result includes:\n", "\n", "- score: The model's confidence in the answer.\n", "- start: The starting position of the answer in the context.\n", "- end: The ending position of the answer in the context.\n", "- answer: The extracted answer from the context.\n", "\n", "\n", "In the given example, the model correctly identified the answer to the question \"Where do I work?\" as Hugging Face. The result also provides the confidence score and the positions of the answer within the context."]}, {"cell_type": "markdown", "metadata": {"id": "98f9fdmZTNac"}, "source": ["## Summarization\n", "\n", "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text."]}, {"cell_type": "markdown", "metadata": {"id": "X12Au6xeTNac"}, "source": ["### Creating the Summarization Pipeline\n", "\n", "This line initializes a pipeline for the summarization task. The \"summarization\" argument specifies that we want to use a model trained to summarize text."]}, {"cell_type": "code", "execution_count": 28, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 264, "referenced_widgets": ["1e0fc18d59e64e0895fa5f85f030c06b", "0ead4af3be54477cb5d839dad5e13c89", "9ec9807d1c244904b35a4d5a69aa942d", "52fc8f09300a4c3ba042b51959731ef0", "cd17c314a40f4ed5a1fbe6579c2622a0", "2ba0eb1f27ec40b5b90ea78b6389ff4a", "2f69deb7c36647ef9309c6c784309af1", "3e4ae2a44e7c46509a363b8d15cdfd72", "9d7d1126d77a4a11ba9eabf6790f1a20", "1753fc39cf1d44a3adafdd1ec699f58b", "2d948b56623e4efbb8b46ee20cbce453", "97e7068a876b44ba8e3b7db5cf6d3abb", "6288939ab7254508bc8732331e48de10", "02f97f7b401c4eab8a7b8e7654a55b9c", "73da90ec35974c33bf80a027713040e2", "3230848648814366a01b72fd7be83a4f", "31156c7132dd4beb9ea898af3954dd65", "9913954554e7427aade2e83155711180", "8d4489f3a8fc4178b167475e30828464", "120984236a274423b94ff1d1cbef5452", "f29cfb76f2584713a8a6614fcf8a1bf0", "cc618210f1fe41c2bfd24eb4db746913", "3adc5918c6f747229b8bbef300792590", "8058f10e44ef450c8fb2f0ffbeedc254", "7c3b2234e3294b68b9701c2913b51748", "87e9efe4b1624bc3a2abc18929683a71", "4594177eb0504b0a9b089e2d36bdc519", "4094513340d242cab8bcb2cf90537505", "a1f037c21c9b4cc1a34bcc87971eb0ee", "9b81dcd5ab87416d8f4e30988c29a062", "570c3c4635ec45a0abe100b503000451", "0817073000cb4fa19442f8e279c70927", "d834948518474bd7909877073bfab242", "8edffd8aa97b4b2a8a5ccc2994ea4a65", "7730df8cf3774e358f58afcbfdd11c82", "4eb5576b9288413fa262d9853b3db5e8", "0eb43445c84f4414a883bfac22bc2688", "a290f8dcb3314caebea0993fa34b7047", "bda69f145e4149119e24f34a6964ba76", "9fc1bf807d584ea79fb3712a3a55b3f0", "2b808ff65ecd4152981e84cf5e51adbf", "415c24f397494d6eb34276cb04d94508", "7e8c247ca04e487cbba88af48712dd9d", "9f60d3bafbe94402a62e8269db9b6c0c", "47ebcf7aa1aa4036a16d701cb5801a9b", "6d16243fc28340e68ca2d694affa7696", "e65a6caab3c84097bfbe506c5f8ee1f6", "8ed7bd7d6dae46e080bd63fd82cc080c", "37127f3268294cb393b2a22ccc08b2ed", "98584daff3ff4c4b81fa99c480ba5dde", "607956adda04467f8dc432a3f9b79c71", "a5ec681ff82443e4a11000af5ce49e8a", "99a7f083593a445e80ba38ae594b59a8", "31fd22e4b8ed48d882657ef07f11f847", "f2b94730a1aa4306935b4c43d324582d", "f95c216830174132a88330e520360c77", "9a5d06a88218437db0d7c78a3694049c", "dcbe03079752404ca53866e2735e1e45", "c2fb137f62974ad0a7e99c529d48540a", "640b54bb001848e7ae0f881ccdf74c3d", "153ef1c9254343e0923679a076751eeb", "f30ace2bba504a1c8982edf91177a437", "b74a4d82c5104e04ba0e4776ce95d2fb", "79ba38bd7cb44101bef72ef2611b6c5e", "9ef1a9e670c64f9b8c5cb5a205f7ebde", "1d0811625cdf419da2b6228035c93b47"]}, "id": "w1JEv0v9TNad", "outputId": "fedf044f-6d18-4dfd-a428-a247c6d58e9d"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "1e0fc18d59e64e0895fa5f85f030c06b", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "97e7068a876b44ba8e3b7db5cf6d3abb", "version_major": 2, "version_minor": 0}, "text/plain": ["pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "3adc5918c6f747229b8bbef300792590", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "8edffd8aa97b4b2a8a5ccc2994ea4a65", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "47ebcf7aa1aa4036a16d701cb5801a9b", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "f95c216830174132a88330e520360c77", "version_major": 2, "version_minor": 0}, "text/plain": ["merges.txt: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n"]}], "source": ["# Create a pipeline for the summarization task\n", "summarizer = pipeline(\"summarization\")"]}, {"cell_type": "markdown", "metadata": {"id": "OAcAwnIcTNad"}, "source": ["### Summarizing the Text\n", "\n", "In this block, we use the summarizer pipeline to condense the provided text into a shorter version while keeping the most important information."]}, {"cell_type": "code", "execution_count": 29, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 55}, "id": "-gmxtXxHTNad", "outputId": "907d5ce6-61c9-464c-9412-c4a438f3575a"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[{'summary_text': ' China and India graduate six and eight times as many traditional engineers as the U.S. as does the United States . Rapidly developing economies such as India and Europe continue to encourage and advance the teaching of engineering . There are declining offerings in engineering subjects dealing with infrastructure, infrastructure, the environment, and related issues, and technology subjects .'}]\n"]}], "source": ["# Use the pipeline to summarize the given text\n", "summary = summarizer(\n", "    \"\"\"\n", "    America has changed dramatically during recent years. Not only has the number\n", "    of graduates in traditional engineering disciplines such as mechanical, civil,\n", "    electrical, chemical, and aeronautical engineering declined, but in most of\n", "    the premier American universities engineering curricula now concentrate on\n", "    and encourage largely the study of engineering science. As a result, there\n", "    are declining offerings in engineering subjects dealing with infrastructure,\n", "    the environment, and related issues, and greater concentration on high\n", "    technology subjects, largely supporting increasingly complex scientific\n", "    developments. While the latter is important, it should not be at the expense\n", "    of more traditional engineering.\n", "\n", "    Rapidly developing economies such as China and India, as well as other\n", "    industrial countries in Europe and Asia, continue to encourage and advance\n", "    the teaching of engineering. Both China and India, respectively, graduate\n", "    six and eight times as many traditional engineers as does the United States.\n", "    Other industrial countries at minimum maintain their output, while America\n", "    suffers an increasingly serious decline in the number of engineering\n", "    graduates and a lack of well-educated engineers.\n", "    \"\"\"\n", ")\n", "\n", "# Display the summary\n", "# The summary provides a condensed version of the original text while retaining the most important information\n", "print(summary)"]}, {"cell_type": "code", "execution_count": 32, "metadata": {"id": "6xIrgH4XTNae"}, "outputs": [], "source": [">>\n", "[{'summary_text': ' America has changed dramatically during recent years . The number of graduates in traditional engineering disciplines has declined . China and India graduate six and eight times as many traditional engineers as does the United States . Rapidly developing economies such as India and Europe continue to encourage and advance the teaching of engineering .'}]\n"]}, {"cell_type": "markdown", "metadata": {"id": "tixYyvvwTNae"}, "source": ["### Control\n", "\n", "Same as text generation:\n", "\n", " - max_length\n", " - min_length"]}, {"cell_type": "markdown", "metadata": {"id": "vfP45I3xTNae"}, "source": ["## Translation\n", "\n", "For translation, you can use a default model if you provide a language pair in the task name (such as \"translation_en_to_fr\"), but the easiest way is to pick the model you want to use on the Model Hub."]}, {"cell_type": "markdown", "metadata": {"id": "KqycO8xRTNaf"}, "source": ["### Creating the Translation Pipeline\n", "\n", "This line initializes a pipeline for the translation task. The \"translation\" argument specifies that we want to use a model trained for translation. The model=\"Helsinki-NLP/opus-mt-fr-en\" argument specifies the specific model to use for translating from French to English."]}, {"cell_type": "code", "execution_count": 33, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 328, "referenced_widgets": ["f9eb7246fabd45b2ade4e07a9a6ccfe9", "1b48dcd0715f4947903a8ed5c3807962", "15bb61af49974cce839231661bcfa7e4", "3f609f8b587f457386f94e97ac14447d", "b5a33a15111d4606b27e5399afeca164", "0a3e41e5ad8744719573e268d677e92a", "a3a643d4957f404f91d3639d8e33ac5b", "cdbfbf3418c7441b9e12c889e58a3a02", "c9f2c315f672473ea5dc6b55d1301f35", "7efd544ae55745c1b5aab01b0a2e8c30", "c41a9a8e8d7d46dbb1a1aa097beaea8c", "6fcdcc216951488f8fd25ba94d02b933", "2eadf82be0b54a65bee25296ae3dc4a8", "ca8ba657ce784beb8a8a9f80e9c03d39", "65b80bc835f744f2a97eacc2dc6d0d9a", "f3d9783338ad45d9ba8b15ff61ad1c2b", "0db32a6607dd4b7ab2bace88591d21af", "a0c88960927b4d1aad9b882df57b4ddd", "60d007eef2184a62b77c446f047994db", "10a84f56ea464dfb9d25a71f5725155f", "e121cc2c6c3546e4ba558a34947a10dc", "aef1c172499d465aa6e555ab6545a468", "43c06548b16b49728c01679fa4672f63", "9424c77ff4904d269954a0164d80cdf3", "1ee0a21929834983a6f691a5ac160b02", "b55b723b6fc44fd59a7416f32f283550", "36554e2f22db4bc388cfbfe67b7805fd", "d62fa66e7fd74a86a2b410b5b4f3cb5d", "600286b969af4f689825f6ee42f4e345", "e498ef7d42e848c898e9c155d8ac832c", "93aca3b3b06343f78986b59c6af03f9f", "2e1cc9a3b0b446269ffbddf08641c4be", "908bdb0ac4024b23ae82037d06639975", "ed2ed75a474a46f5817a13c6d76031f3", "6ed361c569d2419994ac39626748b15e", "344edbf2f19d4f45a747ac6e594ded98", "b3fcfc509d34484a924e273b71aace8a", "69fc9428910f487fa8f9ac11fd84eecf", "0a035400f08b4a0883983d4bab847ed8", "aec335e7421842afa43a6d4e19fade7d", "30df26a90b854801bce8ada431c53263", "9c44327223644778bc16d718a10ae5dd", "4813e8fe849c4cc698774f396c3a643b", "bea355bb3bc14a5f940b1f9866dc7a04", "abba721061544b24b3d0272f36f07e06", "11f5861e03f94adfbdcc92d276029deb", "06bb3a9449594ddb8bd665e4beea6411", "e958d125561a41c28fa294e09bd6b818", "4be2fe33c03540d09b582ff014ab11fa", "36773a84c4c94887981dd1c7c8d01dd7", "487130be4723470ca70e63c4721b0f94", "4756cd03f4c84162bc8ae8bbfd225687", "a817783ea84a484faa475498a21eedba", "5cbe07fb91b940d285ce92a17914b95c", "e15a10b9ae234b07a8d5764e4aa3c22d", "9543b6fde0694c3191665f4c66744b52", "b0ffcc709cb14d0c82f1e4cbc7b5b570", "e2f65d49acff4e3d9a92f47acf70c1b0", "1c9befe08bd84f78b1a0e57296683571", "8b0dd604c7cc43f091194d625467e619", "a777a7fe20e8471195e8069c29ede75a", "4f9a92ee0e4d47688d44cb45aed928d2", "300e95e2c3154193a193ac8a0661f8ee", "4db288c86fcf45e4b2bbbce7939c3145", "d60529c4558e44e5bd0aa362a3e1b614", "abc64982f0a84bcc965d5231405bd661", "4ce3956f6a384ad3ab7496e3561f2dbe", "3eb8f6958dec479f99cd4bc488517796", "c9c3693c5bd74f2e8e17cd49b109a0f1", "e814cfe5bc2546d6a490fd8c5a35fde1", "eef5232d0ecd4bf389bc90be8672dc0e", "d56752c3f3aa406e847489976e47df40", "4c5eb634368a445f9bd009347a1f9f33", "7526a0128f2143a08b2bd3065dffa9ea", "e6a27fd7158a4ed5a8484aafee26bd9c", "bd01ded3e85f4af799418d5212f99f9e", "126723aa97084b5f9da75e613181b286", "be1067dc5eb9446d9e647a22a6597e5d", "fe381732d61f45a784158b43ed992bed", "8e9b458003434aa7a64886ddc14cefc8", "6bd42c2ef71b4232836e80b9d594b63c", "4b9d99a0b455475fb4a9497c67133b2b", "05ee833c799e497bb5576035d9301c66", "7478dfa1afc749788bacdde9ec0df26d", "6d9cc08ef5704503be2a68c63abeda4d", "5ef6052d70c54c76838dd9e26daff7dc", "360cb9b2728c4d549bf5dc8a56afa445", "0d581e0d362844248fb85150905b253c"]}, "id": "qnX6ZB3gTNaf", "outputId": "db76c04f-6ce5-478b-df0e-212aa3007193"}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "f9eb7246fabd45b2ade4e07a9a6ccfe9", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "6fcdcc216951488f8fd25ba94d02b933", "version_major": 2, "version_minor": 0}, "text/plain": ["pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "43c06548b16b49728c01679fa4672f63", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "ed2ed75a474a46f5817a13c6d76031f3", "version_major": 2, "version_minor": 0}, "text/plain": ["generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "abba721061544b24b3d0272f36f07e06", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "9543b6fde0694c3191665f4c66744b52", "version_major": 2, "version_minor": 0}, "text/plain": ["source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "4ce3956f6a384ad3ab7496e3561f2dbe", "version_major": 2, "version_minor": 0}, "text/plain": ["target.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "be1067dc5eb9446d9e647a22a6597e5d", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n", "  warnings.warn(\"Recommended: pip install sacremoses.\")\n", "Device set to use cpu\n"]}], "source": ["# Create a pipeline for the translation task\n", "# Specify the model to be used for translation from French to English\n", "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")"]}, {"cell_type": "markdown", "metadata": {"id": "BJWmUfLrTNag"}, "source": ["### Translating the Text\n", "\n", "In this line, we use the translator pipeline to translate the provided French text into English."]}, {"cell_type": "code", "execution_count": 34, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "IVGyiLzOTNag", "outputId": "d857d457-83ec-4fed-914f-f496b9297eb3"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Translated Text: This course is produced by Hugging Face.\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er le pipeline de traduction (fran\u00e7ais \u2192 anglais)\n", "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n", "\n", "# Texte \u00e0 traduire\n", "text_to_translate = \"Ce cours est produit par Hugging Face.\"\n", "\n", "# Effectuer la traduction\n", "translation_result = translator(text_to_translate)\n", "\n", "# Afficher la traduction\n", "for t in translation_result:\n", "    print(f\"Translated Text: {t['translation_text']}\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "sTiOxm0RTNag"}, "outputs": [], "source": [">>\n", "[{'translation_text': 'This course is produced by Hugging Face.'}]"]}, {"cell_type": "markdown", "metadata": {"id": "g9wf7JQFTNah"}, "source": ["### Control\n", "\n", "Same as text generation & summarization:\n", "\n", "- max_length\n", "- min_length"]}, {"cell_type": "markdown", "metadata": {"id": "kredKbuPTNah"}, "source": ["## Exercices"]}, {"cell_type": "markdown", "metadata": {"id": "QdgNmdQDTNah"}, "source": ["### Exercise 1: Sentiment Analysis\n", "\n", " - Analyze the sentiment of the following sentences: \"I am very happy with the service.\" and \"The food was terrible.\"\n", " - Add a new sentence to the list: \"I'm feeling neutral about this.\" and analyze its sentiment."]}, {"cell_type": "code", "execution_count": 35, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "DGP5Lnvs5b38", "outputId": "7d62ecdf-7add-4fbe-8a97-72156d6f192a"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Text: I am very happy with the service.\n", "Sentiment: POSITIVE, Score: 0.9999\n", "\n", "Text: The food was terrible.\n", "Sentiment: NEGATIVE, Score: 0.9992\n", "\n", "Text: I'm feeling neutral about this.\n", "Sentiment: NEGATIVE, Score: 0.9982\n", "\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er le pipeline pour l'analyse de sentiment\n", "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n", "\n", "# Liste de phrases \u00e0 analyser\n", "texts = [\n", "    \"I am very happy with the service.\",\n", "    \"The food was terrible.\",\n", "    \"I'm feeling neutral about this.\"\n", "]\n", "\n", "# Analyser le sentiment de chaque phrase\n", "results = sentiment_analyzer(texts)\n", "\n", "# Afficher les r\u00e9sultats\n", "for text, result in zip(texts, results):\n", "    print(f\"Text: {text}\")\n", "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\\n\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "k6cgv8WFTNai"}, "source": ["### Exercise 2: Zero-shot Classification\n", "\n", " - Classify the following text into one of the candidate labels: \"finance\", \"healthcare\", \"politics\": \"The new policy aims to improve healthcare accessibility for all citizens.\"\n", " - Change the candidate labels to \"education\", \"entertainment\", \"business\" and classify the same text."]}, {"cell_type": "code", "execution_count": 36, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "wxOU572Y5ivd", "outputId": "863e1276-37c6-43e8-806f-1bce43a28e26"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Classification with labels: ['finance', 'healthcare', 'politics'] ===\n", "Text: The new policy aims to improve healthcare accessibility for all citizens.\n", "Classification: {'sequence': 'The new policy aims to improve healthcare accessibility for all citizens.', 'labels': ['healthcare', 'politics', 'finance'], 'scores': [0.9620512127876282, 0.027672862634062767, 0.010275942273437977]}\n", "\n", "=== Classification with labels: ['education', 'entertainment', 'business'] ===\n", "Text: The new policy aims to improve healthcare accessibility for all citizens.\n", "Classification: {'sequence': 'The new policy aims to improve healthcare accessibility for all citizens.', 'labels': ['business', 'entertainment', 'education'], 'scores': [0.5286611318588257, 0.30794841051101685, 0.16339050233364105]}\n", "\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er le pipeline pour la classification zero-shot\n", "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n", "\n", "# Texte \u00e0 classifier\n", "text = \"The new policy aims to improve healthcare accessibility for all citizens.\"\n", "\n", "# Premier ensemble d'\u00e9tiquettes\n", "candidate_labels_1 = [\"finance\", \"healthcare\", \"politics\"]\n", "result_1 = zero_shot_classifier(text, candidate_labels=candidate_labels_1)\n", "print(\"=== Classification with labels:\", candidate_labels_1, \"===\")\n", "print(f\"Text: {text}\")\n", "print(f\"Classification: {result_1}\\n\")\n", "\n", "# Deuxi\u00e8me ensemble d'\u00e9tiquettes\n", "candidate_labels_2 = [\"education\", \"entertainment\", \"business\"]\n", "result_2 = zero_shot_classifier(text, candidate_labels=candidate_labels_2)\n", "print(\"=== Classification with labels:\", candidate_labels_2, \"===\")\n", "print(f\"Text: {text}\")\n", "print(f\"Classification: {result_2}\\n\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "JhHpASraTNai"}, "source": ["### Exercise 3: Text Generation\n", "\n", " - Generate text with the prompt \"Artificial intelligence will change the world by\" with a maximum length of 50 tokens.\n", " - Change the prompt to \"In the future, we will see advancements in\" and generate text with a maximum length of 30 tokens."]}, {"cell_type": "code", "execution_count": 37, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "CHV6ybIs5xOn", "outputId": "7ad4a3a4-bb71-4137-91d0-64c568ce93f4"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n", "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n", "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n", "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n", "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n", "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Generated Text for Prompt 1 ===\n", "Generated Text 1: Artificial intelligence will change the world by the end of the decade as we learn more about the artificial intelligence.\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "=== Generated Text for Prompt 2 ===\n", "Generated Text 1: In the future, we will see advancements in the technology. We will see technological innovation in the way we use it and, in the future, we will see more innovations. We will see other innovations. We will see smart cars. We will see the advancements in technology. We will see people learning to drive. We will see technology. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning to drive. We will see people learning\n", "\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er le pipeline de g\u00e9n\u00e9ration de texte avec le mod\u00e8le distilgpt2\n", "text_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n", "\n", "# --- Premier prompt ---\n", "prompt_1 = \"Artificial intelligence will change the world by\"\n", "generated_1 = text_generator(prompt_1, max_length=50, num_return_sequences=1)\n", "\n", "print(\"=== Generated Text for Prompt 1 ===\")\n", "for i, text in enumerate(generated_1):\n", "    print(f\"Generated Text {i+1}: {text['generated_text']}\\n\")\n", "\n", "# --- Deuxi\u00e8me prompt ---\n", "prompt_2 = \"In the future, we will see advancements in\"\n", "generated_2 = text_generator(prompt_2, max_length=30, num_return_sequences=1)\n", "\n", "print(\"=== Generated Text for Prompt 2 ===\")\n", "for i, text in enumerate(generated_2):\n", "    print(f\"Generated Text {i+1}: {text['generated_text']}\\n\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "xVSugTkjTNaj"}, "source": ["### Exercise 4: Mask Filling\n", "\n", " - Predict the masked word in the sentence \"Artificial intelligence is the future of <mask>.\"\n", " - Change the sentence to \"The development of AI will revolutionize <mask>.\" and predict the masked word."]}, {"cell_type": "code", "execution_count": 38, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 454, "referenced_widgets": ["f6bf123b74cc4398baf9bbbf6bbd0b88", "e1329e15f8734bc88400a7477bc1ec7a", "e217479ff7774320a75710ace1aeadb4", "d7bd807296484261a416511985ec8fd4", "9193b98c6fe34e53b84aa84e0d7456ba", "a089d766cd4c434d89325b508c088d4d", "c67389f970984794b85f233b836193a9", "29306950d06042bf8d5bbbd872f644e0", "244cdd5d6114494e94c55c28ab496628", "29ce65def05d49a2ab923171f4d1b4b0", "285e3e78f1ef41b3ab1bc5722386b784", "835156e90f0e406a8f858cd1e29aa1ef", "f3c3d04726cf4ca0b4413aae23dd63d7", "0d28273057654e7a97e2afa2dbfb6747", "32d616ddf27f4f2b9085eea02d85930e", "93aa34228e704297882b631e80d2fcaa", "fd7a09ba6ce14c958f539cd0b3a90f4e", "f1447e2a6da744628dbd5575a17597f0", "355ee114758a4030b10748b2b8cc2e57", "3cec97585e2445d9952c8cfdcded1776", "67305bc23a014f2584264f5e96990c7d", "58396fa10e0a431db740e78e454b9300", "861c3e6b03ba4ac188ee76ed1a1b5413", "80f6f40d7eb449c1beed08d27cbcdfb6", "519d7aa8284f4163845e73313eda0e7b", "421fc906dc254147be4da2df72a0f800", "41fb59b0b7bd472690ee34b81a04ae9f", "957f89526c514482a4677e9417465d7e", "3999b07904fc4b849c3c7cd5985c4f9e", "c5987f98fd0246569494ea954f450327", "0743f26f082440369999397617d10ab1", "cdb55ae5d18a40159d580c1dc655bb78", "4ad8567ddb7645caa99f0d63a0ad7bad", "abe5dd812ce149b3a284db3d67ecc1a2", "0c3e289302b941d194b92f76471ed16d", "dd862ac4398c43b1a527d02ca52be25a", "b666d2fc293b4e7ba0b39a6a90fd04db", "6ad1d56853a241578e73d916074f6ae8", "94c8d253d36142c3a1d01d537e2dc1f2", "be63f6e004fc42ee974e61fcf2acd7fb", "d8086080e90346d6a8567d0e4d00486d", "3d04f3f01bb74d9a9aaa25e1c9cd43b6", "f1be5dc5389e42ff9686ea3335b1b4ce", "bdca96472ea043929c859a3c77208cfe", "fdbc12ff2718470ea715d2752bf88f39", "e89cad46c0c04b989a38c29e706dea50", "463dcc66decb405590ce4ca0e4b39027", "7b64f583cecc4d7c8bd01359aa699c1c", "1b6edd0cbc494a688124dc8dfa8c28f4", "99c8a98ab7bc4a1eb4786a7205c21542", "caeb4b5ec1bb460bb899cf633b361d50", "9b8763bdac524a02ae04188ee618c543", "5a6b56ebda9d48a6abaff70e9be35104", "31ac482b21d244fda39f6b6c42bda59c", "66bd11f180234a85a862c5879997ea02"]}, "id": "PJQggB1_6DsM", "outputId": "cda0a9b7-dc25-40a8-ff00-b3cd31bde931"}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "f6bf123b74cc4398baf9bbbf6bbd0b88", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "835156e90f0e406a8f858cd1e29aa1ef", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n", "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n", "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "861c3e6b03ba4ac188ee76ed1a1b5413", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "abe5dd812ce149b3a284db3d67ecc1a2", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "fdbc12ff2718470ea715d2752bf88f39", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Masked Predictions for Sentence 1 ===\n", "Token: science, Score: 0.2191, Sequence: artificial intelligence is the future of science.\n", "Token: technology, Score: 0.0878, Sequence: artificial intelligence is the future of technology.\n", "Token: society, Score: 0.0717, Sequence: artificial intelligence is the future of society.\n", "\n", "\n", "=== Masked Predictions for Sentence 2 ===\n", "Token: ai, Score: 0.1091, Sequence: the development of ai will revolutionize ai.\n", "Token: society, Score: 0.0741, Sequence: the development of ai will revolutionize society.\n", "Token: technology, Score: 0.0474, Sequence: the development of ai will revolutionize technology.\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er le pipeline pour le remplissage de masque (Masked Language Modeling)\n", "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n", "\n", "# --- Premier exemple ---\n", "sentence_1 = \"Artificial intelligence is the future of [MASK].\"\n", "predictions_1 = mask_filler(sentence_1, top_k=3)\n", "\n", "print(\"=== Masked Predictions for Sentence 1 ===\")\n", "for pred in predictions_1:\n", "    print(f\"Token: {pred['token_str']}, Score: {pred['score']:.4f}, Sequence: {pred['sequence']}\")\n", "\n", "print(\"\\n\")\n", "\n", "# --- Deuxi\u00e8me exemple ---\n", "sentence_2 = \"The development of AI will revolutionize [MASK].\"\n", "predictions_2 = mask_filler(sentence_2, top_k=3)\n", "\n", "print(\"=== Masked Predictions for Sentence 2 ===\")\n", "for pred in predictions_2:\n", "    print(f\"Token: {pred['token_str']}, Score: {pred['score']:.4f}, Sequence: {pred['sequence']}\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "mxEpShixTNaj"}, "source": ["### Exercise 5: Named Entity Recognition (NER)\n", "\n", " - Identify entities in the sentence \"Elon Musk founded SpaceX and Tesla.\"\n", " - Add a new sentence \"Barack Obama was the 44th President of the United States.\" and identify entities."]}, {"cell_type": "code", "execution_count": 39, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "FN7fj_3h6m1k", "outputId": "e1147270-b7e1-40fc-c0c5-8196d05fd9ef"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n", "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n", "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n", "Device set to use cpu\n", "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n", "  warnings.warn(\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Named Entities for Sentence 1 ===\n", "Entity: Elon Musk, Type: PER, Score: 0.9971\n", "Entity: SpaceX, Type: ORG, Score: 0.9985\n", "Entity: Tesla, Type: ORG, Score: 0.9917\n", "\n", "\n", "=== Named Entities for Sentence 2 ===\n", "Entity: Barack Obama, Type: PER, Score: 0.9991\n", "Entity: United States, Type: LOC, Score: 0.9952\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er le pipeline pour la reconnaissance d'entit\u00e9s nomm\u00e9es (NER)\n", "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n", "\n", "# --- Premier exemple ---\n", "sentence_1 = \"Elon Musk founded SpaceX and Tesla.\"\n", "entities_1 = ner_pipeline(sentence_1)\n", "\n", "print(\"=== Named Entities for Sentence 1 ===\")\n", "for entity in entities_1:\n", "    print(f\"Entity: {entity['word']}, Type: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n", "\n", "print(\"\\n\")\n", "\n", "# --- Deuxi\u00e8me exemple ---\n", "sentence_2 = \"Barack Obama was the 44th President of the United States.\"\n", "entities_2 = ner_pipeline(sentence_2)\n", "\n", "print(\"=== Named Entities for Sentence 2 ===\")\n", "for entity in entities_2:\n", "    print(f\"Entity: {entity['word']}, Type: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "qH69yC9wTNak"}, "source": ["### Exercise 6: Question Answering\n", "\n", " - Answer the question \"What is the name of the company?\" given the context \"Amazon is a global company based in Seattle.\"\n", " - Change the context to \"Google was founded by Larry Page and Sergey Brin.\" and ask the question \"Who founded Google?\""]}, {"cell_type": "code", "execution_count": 40, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "3Cv-gWqp63em", "outputId": "61dffde7-4caa-4e69-ace3-5d13d97089d6"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Question Answering Example 1 ===\n", "Question: What is the name of the company?\n", "Context: Amazon is a global company based in Seattle.\n", "Answer: Amazon, Score: 0.9975\n", "\n", "=== Question Answering Example 2 ===\n", "Question: Who founded Google?\n", "Context: Google was founded by Larry Page and Sergey Brin.\n", "Answer: Larry Page and Sergey Brin, Score: 0.8125\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er le pipeline pour Question Answering\n", "qa_pipeline = pipeline(\"question-answering\")\n", "\n", "# --- Premier exemple ---\n", "context_1 = \"Amazon is a global company based in Seattle.\"\n", "question_1 = \"What is the name of the company?\"\n", "\n", "answer_1 = qa_pipeline(question=question_1, context=context_1)\n", "print(\"=== Question Answering Example 1 ===\")\n", "print(f\"Question: {question_1}\")\n", "print(f\"Context: {context_1}\")\n", "print(f\"Answer: {answer_1['answer']}, Score: {answer_1['score']:.4f}\\n\")\n", "\n", "# --- Deuxi\u00e8me exemple ---\n", "context_2 = \"Google was founded by Larry Page and Sergey Brin.\"\n", "question_2 = \"Who founded Google?\"\n", "\n", "answer_2 = qa_pipeline(question=question_2, context=context_2)\n", "print(\"=== Question Answering Example 2 ===\")\n", "print(f\"Question: {question_2}\")\n", "print(f\"Context: {context_2}\")\n", "print(f\"Answer: {answer_2['answer']}, Score: {answer_2['score']:.4f}\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "89r3ORH3TNak"}, "source": ["### Exercise 7: Summarization\n", "\n", " - Summarize the following text: \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns, and make decisions with minimal human intervention.\"\n", " - Summarize the following text: \"Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data. Like a human, the algorithm learns from examples. While traditional machine learning algorithms are linear, deep learning algorithms are stacked in a hierarchy of increasing complexity and abstraction.\""]}, {"cell_type": "code", "execution_count": 41, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "MQyG12eH7GWz", "outputId": "e69fbd85-fd02-4c89-e0a6-8736a8e1c3e7"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Device set to use cpu\n", "Your max_length is set to 50, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Summarization Example 1 ===\n", "Original Text: Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns, and make decisions with minimal human intervention.\n", "\n", "Summary:  Machine learning is a method of data analysis that automates analytical model building . It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns, and make decisions with minimal human intervention .\n", "\n", "=== Summarization Example 2 ===\n", "Original Text: Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data. Like a human, the algorithm learns from examples. While traditional machine learning algorithms are linear, deep learning algorithms are stacked in a hierarchy of increasing complexity and abstraction.\n", "\n", "Summary:  Deep learning is a subset of machine learning where artificial neural networks learn from large amounts of data . Deep learning algorithms are stacked in a hierarchy of increasing complexity and abstraction .\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er le pipeline pour la summarization\n", "summarizer = pipeline(\"summarization\")\n", "\n", "# --- Premier texte ---\n", "text_1 = (\n", "    \"Machine learning is a method of data analysis that automates analytical model building. \"\n", "    \"It is a branch of artificial intelligence based on the idea that systems can learn from data, \"\n", "    \"identify patterns, and make decisions with minimal human intervention.\"\n", ")\n", "\n", "summary_1 = summarizer(text_1, max_length=50, min_length=25, do_sample=False)[0]\n", "print(\"=== Summarization Example 1 ===\")\n", "print(f\"Original Text: {text_1}\\n\")\n", "print(f\"Summary: {summary_1['summary_text']}\\n\")\n", "\n", "# --- Deuxi\u00e8me texte ---\n", "text_2 = (\n", "    \"Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, \"\n", "    \"learn from large amounts of data. Like a human, the algorithm learns from examples. While traditional machine learning \"\n", "    \"algorithms are linear, deep learning algorithms are stacked in a hierarchy of increasing complexity and abstraction.\"\n", ")\n", "\n", "summary_2 = summarizer(text_2, max_length=60, min_length=30, do_sample=False)[0]\n", "print(\"=== Summarization Example 2 ===\")\n", "print(f\"Original Text: {text_2}\\n\")\n", "print(f\"Summary: {summary_2['summary_text']}\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "RjCidywYTNal"}, "source": ["### Exercise 8: Translation\n", "\n", " - Translate the sentence \"Bonjour tout le monde.\" from French to English.\n", " - Translate the sentence \"La technologie transforme notre monde.\" from French to English."]}, {"cell_type": "code", "execution_count": 42, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Ax30zl5u7Rmb", "outputId": "6d4bcf3d-b6a9-441b-949f-883171ebca96"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Translation Example 1 ===\n", "Original Text: Bonjour tout le monde.\n", "Translation: Hello, everybody.\n", "\n", "=== Translation Example 2 ===\n", "Original Text: La technologie transforme notre monde.\n", "Translation: Technology is transforming our world.\n"]}], "source": ["from transformers import pipeline\n", "\n", "# Cr\u00e9er le pipeline pour la traduction (fran\u00e7ais -> anglais)\n", "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n", "\n", "# --- Premier texte ---\n", "text_1 = \"Bonjour tout le monde.\"\n", "translation_1 = translator(text_1, max_length=40)[0]\n", "print(\"=== Translation Example 1 ===\")\n", "print(f\"Original Text: {text_1}\")\n", "print(f\"Translation: {translation_1['translation_text']}\\n\")\n", "\n", "# --- Deuxi\u00e8me texte ---\n", "text_2 = \"La technologie transforme notre monde.\"\n", "translation_2 = translator(text_2, max_length=50)[0]\n", "print(\"=== Translation Example 2 ===\")\n", "print(f\"Original Text: {text_2}\")\n", "print(f\"Translation: {translation_2['translation_text']}\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "a3NHmEDUTNal"}, "source": ["### Exercise 9: Comprehensive NLP Task\n", "\n", "This exercise will combine sentiment analysis, zero-shot classification, and text generation.\n", "\n", " - Perform sentiment analysis on the following sentences: \"I love the new features of this product.\" and \"This is the worst experience I've ever had.\"\n", " - Classify the following text into one of the candidate labels: \"technology\", \"customer service\", \"product quality\": \"The new smartphone has several innovative features that are very user-friendly.\"\n", " - Generate text with the prompt \"Based on the customer feedback, we can improve our product by\" with a maximum length of 50 tokens."]}, {"cell_type": "code", "execution_count": 45, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ZsHkn3W_7qqT", "outputId": "05c06d84-446c-49d3-be38-1a0794f392fb"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Sentiment Analysis ===\n"]}, {"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Text: I love the new features of this product.\n", "Sentiment: POSITIVE, Score: 0.9999\n", "\n", "Text: This is the worst experience I've ever had.\n", "Sentiment: NEGATIVE, Score: 0.9998\n", "\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Zero-Shot Classification ===\n", "Text: The new smartphone has several innovative features that are very user-friendly.\n", "Classification: {'sequence': 'The new smartphone has several innovative features that are very user-friendly.', 'labels': ['technology', 'product quality', 'customer service'], 'scores': [0.8073760867118835, 0.1833198368549347, 0.009304080158472061]}\n", "\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n", "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n", "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n", "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Text Generation ===\n", "Generated Text 1: Based on the customer feedback, we can improve our product by releasing a product that features a more optimized UI, more features, and better functionality.\u201d\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Generated Text 2: Based on the customer feedback, we can improve our product by enhancing our customer service and providing more customer service to customers.\n"]}], "source": ["from transformers import pipeline\n", "\n", "# --- 1\ufe0f\u20e3 Sentiment Analysis ---\n", "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n", "\n", "sentences = [\n", "    \"I love the new features of this product.\",\n", "    \"This is the worst experience I've ever had.\"\n", "]\n", "\n", "print(\"=== Sentiment Analysis ===\")\n", "sentiment_results = sentiment_analyzer(sentences)\n", "for text, result in zip(sentences, sentiment_results):\n", "    print(f\"Text: {text}\")\n", "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\\n\")\n", "\n", "\n", "# --- 2\ufe0f\u20e3 Zero-Shot Classification ---\n", "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n", "\n", "text_to_classify = \"The new smartphone has several innovative features that are very user-friendly.\"\n", "candidate_labels = [\"technology\", \"customer service\", \"product quality\"]\n", "\n", "classification_result = zero_shot_classifier(text_to_classify, candidate_labels=candidate_labels)\n", "print(\"=== Zero-Shot Classification ===\")\n", "print(f\"Text: {text_to_classify}\")\n", "print(f\"Classification: {classification_result}\\n\")\n", "\n", "\n", "# --- 3\ufe0f\u20e3 Text Generation ---\n", "text_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n", "\n", "prompt = \"Based on the customer feedback, we can improve our product by\"\n", "generated_texts = text_generator(prompt, max_length=50, num_return_sequences=2)\n", "\n", "print(\"=== Text Generation ===\")\n", "for i, gen in enumerate(generated_texts):\n", "    print(f\"Generated Text {i+1}: {gen['generated_text']}\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "1UQquzoxTNal"}, "source": ["### Exercise 10: Multi-Function NLP Task\n", "\n", "This exercise will combine named entity recognition (NER), question answering, and text summarization.\n", "\n", " - Identify entities in the sentence: \"Sundar Pichai is the CEO of Google, which is headquartered in Mountain View, California.\"\n", " - Answer the question \"Where is Google headquartered?\" given the context \"Sundar Pichai is the CEO of Google, which is headquartered in Mountain View, California.\"\n", " - Summarize the following text: \"Google, a multinational technology company specializing in Internet-related services and products, was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University.\""]}, {"cell_type": "code", "execution_count": 44, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "MBKU_ZJi7yZ4", "outputId": "aeff80be-3f8a-4d1c-a258-331491c74bb3"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n", "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n", "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n", "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n", "Device set to use cpu\n", "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Named Entity Recognition ===\n", "{'entity_group': 'PER', 'score': np.float32(0.99594295), 'word': 'Sundar Pichai', 'start': 0, 'end': 13}\n", "{'entity_group': 'ORG', 'score': np.float32(0.99887437), 'word': 'Google', 'start': 28, 'end': 34}\n", "{'entity_group': 'LOC', 'score': np.float32(0.99463844), 'word': 'Mountain View', 'start': 62, 'end': 75}\n", "{'entity_group': 'LOC', 'score': np.float32(0.99826956), 'word': 'California', 'start': 77, 'end': 87}\n", "\n", "\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n", "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n", "Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Question Answering ===\n", "Question: Where is Google headquartered?\n", "Answer: Mountain View, California\n", "\n", "\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Device set to use cpu\n", "Your max_length is set to 50, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Summarization ===\n", "Original Text: Google, a multinational technology company specializing in Internet-related services and products, was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University.\n", "Summary:  Google was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University . Google is a multinational technology company specializing in Internet-related services and products .\n"]}], "source": ["from transformers import pipeline\n", "\n", "# --- 1\ufe0f\u20e3 Named Entity Recognition (NER) ---\n", "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n", "ner_text = \"Sundar Pichai is the CEO of Google, which is headquartered in Mountain View, California.\"\n", "ner_results = ner_pipeline(ner_text)\n", "\n", "print(\"=== Named Entity Recognition ===\")\n", "for entity in ner_results:\n", "    print(entity)\n", "print(\"\\n\")\n", "\n", "\n", "# --- 2\ufe0f\u20e3 Question Answering ---\n", "qa_pipeline = pipeline(\"question-answering\")\n", "qa_context = \"Sundar Pichai is the CEO of Google, which is headquartered in Mountain View, California.\"\n", "qa_question = \"Where is Google headquartered?\"\n", "qa_result = qa_pipeline(question=qa_question, context=qa_context)\n", "\n", "print(\"=== Question Answering ===\")\n", "print(f\"Question: {qa_question}\")\n", "print(f\"Answer: {qa_result['answer']}\")\n", "print(\"\\n\")\n", "\n", "\n", "# --- 3\ufe0f\u20e3 Text Summarization ---\n", "summarizer_pipeline = pipeline(\"summarization\")\n", "summarization_text = (\"Google, a multinational technology company specializing in Internet-related services \"\n", "                      \"and products, was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University.\")\n", "summary_result = summarizer_pipeline(summarization_text, max_length=50, min_length=25, do_sample=False)\n", "\n", "print(\"=== Summarization ===\")\n", "print(f\"Original Text: {summarization_text}\")\n", "print(f\"Summary: {summary_result[0]['summary_text']}\")\n"]}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.3"}}, "nbformat": 4, "nbformat_minor": 0}